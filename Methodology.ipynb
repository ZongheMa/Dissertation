{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-02T04:19:43.271734Z",
     "start_time": "2023-08-02T04:19:43.046968Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/geospatial/lib/python3.8/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/var/folders/1k/27mkp8bj3ps60c3nmr7rbqzh0000gn/T/ipykernel_14570/1895528008.py:1: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import IPython.display as display\n",
    "import copy\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def merge_csv_files(directory):\n",
    "    # Get a list of all the csv files\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "    # Initialize an empty list to hold dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through csv files, read each into a dataframe, and append to the list\n",
    "    for file in csv_files:\n",
    "        # Extract date from filename, assuming the date is in format 'traffic_flow_YYYY_MM_DD'\n",
    "        date_str = file.split('.')[0].split('_')[-3:]  # This gives ['YYYY', 'MM', 'DD']\n",
    "        date = datetime.strptime('_'.join(date_str), '%Y_%m_%d').date()\n",
    "\n",
    "        df = pd.read_csv(os.path.join(directory, file))\n",
    "\n",
    "        # Add date as a new column\n",
    "        df['date'] = date.strftime('%m/%d/%y')\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all dataframes in the list into one dataframe\n",
    "    merged_df = pd.concat(dfs, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # Return the merged dataframe\n",
    "    return merged_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T04:19:47.712987Z",
     "start_time": "2023-08-02T04:19:47.709222Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "traffic_flows = merge_csv_files(\n",
    "    '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/[XH]Traffic flow')\n",
    "road_network = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/[XH]road_network/road_network.shp'\n",
    "\n",
    "# clean the traffic flow data\n",
    "traffic_flows = traffic_flows.drop_duplicates(['toid', 'date'])\n",
    "traffic_flows = traffic_flows.groupby(['toid', 'date']).agg(\n",
    "    {'bus': 'sum', 'car': 'sum', 'cycle': 'sum', 'walks': 'sum', 'stationary': 'sum'}).reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T04:20:00.573631Z",
     "start_time": "2023-08-02T04:19:49.466700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/geospatial/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3445: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "lsoa = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/London administrative boundaries/london_LSOA/london_LSOA.shp'\n",
    "road_network = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/[XH]road_network/road_network.shp'\n",
    "inoutter = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/London administrative boundaries/lp-consultation-oct-2009-inner-outer-london-shp/lp-consultation-oct-2009-inner-outer-london.shp'\n",
    "tube_line = 'https://raw.githubusercontent.com/oobrien/vis/master/tubecreature/data/tfl_lines.json'\n",
    "tube_station = 'https://raw.githubusercontent.com/oobrien/vis/master/tubecreature/data/tfl_stations.json'\n",
    "\n",
    "inoutter = gpd.read_file(inoutter)\n",
    "inoutter.to_crs(epsg=27700, inplace=True)\n",
    "\n",
    "# tube_station = gpd.read_file(tube_station)\n",
    "# tube_station.to_crs(epsg=27700, inplace=True)\n",
    "# tube_station = gpd.sjoin(tube_station, inoutter, op='within')\n",
    "\n",
    "tube_line = gpd.read_file(tube_line)\n",
    "tube_line.to_crs(epsg=27700, inplace=True)\n",
    "tube_line = gpd.sjoin(tube_line, inoutter, op='within')\n",
    "\n",
    "lsoa = gpd.read_file(lsoa, crs={'init': 'epsg:27700'})\n",
    "road_network = gpd.read_file(road_network, crs={'init': 'epsg:27700'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T04:21:21.251954Z",
     "start_time": "2023-08-02T04:20:50.151402Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# clean the traffic flow data\n",
    "traffic_flows = traffic_flows.drop_duplicates(['toid', 'date'])\n",
    "traffic_flows = traffic_flows.groupby(['toid', 'date']).agg(\n",
    "    {'bus': 'sum', 'car': 'sum', 'cycle': 'sum', 'walks': 'sum', 'stationary': 'sum'}).reset_index()\n",
    "traffic_flows['total'] = traffic_flows['bus'] + traffic_flows['car'] + traffic_flows['cycle'] + traffic_flows[\n",
    "    'walks'] + traffic_flows['stationary']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T04:22:10.655568Z",
     "start_time": "2023-08-02T04:22:07.484604Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "flows = pd.merge(\n",
    "    road_network[['toid', 'roadclassi', 'geometry', 'directiona', 'length', 'roadwidthm', 'elevationg'\n",
    "                  ]],\n",
    "    traffic_flows, left_on='toid', right_on='toid', how='right')\n",
    "\n",
    "flows['classification'] = flows['roadclassi'].replace(\n",
    "    {'Unknown': 'Local Road', 'Not Classified': 'Local Road', 'Unclassified': 'Local Road',\n",
    "     'Classified Unnumbered': 'Local Road'})\n",
    "\n",
    "flows.drop(columns=['roadclassi'], inplace=True)\n",
    "\n",
    "stage_date = ['03/01/22', '02/22/22', '03/08/22']\n",
    "flows = flows.loc[flows['date'].isin(stage_date)]\n",
    "\n",
    "# label the regional level\n",
    "flows = gpd.sjoin(flows, inoutter, how='inner', predicate='within')\n",
    "flows = flows.drop(columns=['index_right', 'Source', 'Area_Ha', 'Shape_Leng', 'Shape_Area'])\n",
    "\n",
    "# convert the dataframe\n",
    "flows = pd.melt(flows,\n",
    "                id_vars=['toid', 'classification', 'geometry', 'directiona', 'length', 'roadwidthm', 'elevationg',\n",
    "                         'date', 'Boundary'], var_name='mode', value_name='flow')\n",
    "\n",
    "flows = pd.pivot_table(flows,\n",
    "                       index=['toid', 'classification', 'geometry', 'directiona', 'length', 'roadwidthm', 'elevationg',\n",
    "                              'date', 'Boundary', 'mode'], columns='date', values='flow', aggfunc='first').reset_index()\n",
    "\n",
    "flows.drop(columns=['date'], inplace=True)\n",
    "flows = flows.groupby(\n",
    "    ['toid', 'mode', 'classification', 'geometry', 'directiona', 'length', 'roadwidthm', 'elevationg',\n",
    "     'Boundary'], as_index=False).agg(\n",
    "    {'03/01/22': 'first', '02/22/22': 'first', '03/08/22': 'first'}).reset_index()\n",
    "\n",
    "# calculate the impact and recovery flows for one strike\n",
    "flows['impact_flow'] = flows['03/01/22'] - flows['02/22/22']\n",
    "flows['recovery_flow'] = flows['03/08/22'] - flows['03/01/22']\n",
    "\n",
    "flows = flows.drop(columns=['index'],inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T04:41:03.010719Z",
     "start_time": "2023-08-02T04:38:31.401244Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### K-means clustering"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T04:34:20.691276Z",
     "start_time": "2023-08-02T04:34:20.443610Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Group the DataFrame by unique values in 'mode' column\n",
    "grouped = flows.groupby('mode')\n",
    "\n",
    "# Perform KMeans clustering for each group separately\n",
    "for mode, group in grouped:\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    flows.loc[group.index, 'KmeansBymode_impact'] = kmeans.fit_predict(group[['impact_flow']])\n",
    "    flows.loc[group.index, 'KmeansBymode_recovery'] = kmeans.fit_predict(group[['recovery_flow']])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-02T04:37:52.420491Z",
     "start_time": "2023-08-02T04:37:52.181684Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a scatter plot to visualize 'KmeansBymode_impact' and its clustering results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=flows, x='KmeansBymode_impact', y='impact_flow', hue='mode', palette='tab10', s=100)\n",
    "plt.title('KMeans Clustering for Different Modes')\n",
    "plt.xlabel('KmeansBymode_impact')\n",
    "plt.ylabel('Impact Flow')\n",
    "plt.legend(title='Mode', loc='upper right', bbox_to_anchor=(1.15, 1.0))\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_kmeans_by_mode_and_classification(flows, feature1, feature2):\n",
    "    \"\"\"\n",
    "    Visualize KMeans clustering results for different 'mode' and 'classification' categories.\n",
    "\n",
    "    Parameters:\n",
    "        flows (DataFrame): The DataFrame containing the data.\n",
    "        feature1 (str): The name of the first feature to visualize KMeans clusters.\n",
    "        feature2 (str): The name of the second feature to visualize KMeans clusters.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a new column to store the combined KMeans cluster labels\n",
    "    flows['KmeansByModeAndClassification'] = None\n",
    "\n",
    "    # Group the DataFrame by unique values in 'mode' column\n",
    "    grouped_by_mode = flows.groupby('mode')\n",
    "\n",
    "    # Perform KMeans clustering for each 'mode' group separately\n",
    "    for mode, group_mode in grouped_by_mode:\n",
    "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "        flows.loc[group_mode.index, 'KmeansByModeAndClassification'] = kmeans.fit_predict(group_mode[[feature1, feature2]])\n",
    "\n",
    "        # Create subplots for each 'classification' category\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(data=flows.loc[group_mode.index], x=feature1, y=feature2, hue='KmeansByModeAndClassification', palette='tab10', s=100)\n",
    "        plt.title(f'KMeans Clustering for {mode} Mode')\n",
    "        plt.xlabel(feature1)\n",
    "        plt.ylabel(feature2)\n",
    "        plt.legend(title='KMeans Cluster')\n",
    "        plt.show()\n",
    "\n",
    "# Call the visualization method\n",
    "visualize_kmeans_by_mode_and_classification(flows, 'impact_flow', 'recovery_flow')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设你的DataFrame名为flows，包含'mode'、'toid'、'classification'和'impact_flow'等列\n",
    "\n",
    "# 定义分类顺序\n",
    "classification_order = ['Motorway', 'A Road', 'B Road', 'Local Road']\n",
    "mode_order = ['total', 'car', 'bus', 'cycle', 'walks', 'stationary']\n",
    "\n",
    "# 使用pivot_table将'mode'作为列，'classification'作为行，'impact_flow'作为值，并进行求和（sum）\n",
    "flows_pivot = pd.pivot_table(flows, index='classification', columns='mode', values='impact_flow', aggfunc='sum')\n",
    "\n",
    "# 对DataFrame进行排序，以调整顺序\n",
    "flows_pivot = flows_pivot.reindex(classification_order, axis=0)\n",
    "flows_pivot = flows_pivot[mode_order]\n",
    "\n",
    "# 对impact_flow取对数\n",
    "flows_pivot = np.log(flows_pivot)\n",
    "\n",
    "# 绘制条形图\n",
    "plt.figure(figsize=(10, 6))\n",
    "flows_pivot.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Impact Flow by Classification and Mode')\n",
    "plt.xlabel('Classification')\n",
    "plt.ylabel('Impact Flow')\n",
    "plt.legend(title='Mode', loc='upper right', bbox_to_anchor=(1.15, 1.0))\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义分类顺序和模式顺序\n",
    "classification_order = ['Motorway', 'A Road', 'B Road', 'Local Road']\n",
    "mode_order = ['total', 'car', 'bus', 'cycle', 'walks', 'stationary']\n",
    "\n",
    "# 使用pivot_table将'mode'作为列，'classification'作为行，'impact_flow'作为值，并进行求和（sum）\n",
    "flows_pivot = pd.pivot_table(flows, index='classification', columns='mode', values='impact_flow', aggfunc='sum')\n",
    "\n",
    "# 对DataFrame进行排序，以调整顺序\n",
    "flows_pivot = flows_pivot.reindex(classification_order, axis=0)\n",
    "flows_pivot = flows_pivot[mode_order]\n",
    "\n",
    "# 对impact_flow取对数\n",
    "flows_pivot_log = np.log(flows_pivot)\n",
    "\n",
    "# 根据'KmeansBymode_impact'列的值将flows DataFrame分为两部分\n",
    "flows_impact_1 = flows[flows['KmeansBymode_impact'] == 1]\n",
    "flows_impact_0 = flows[flows['KmeansBymode_impact'] == 0]\n",
    "\n",
    "# 将flows DataFrame转换为geopandas GeoDataFrame\n",
    "gdf_impact_1 = gpd.GeoDataFrame(flows_impact_1, geometry='geometry')\n",
    "gdf_impact_0 = gpd.GeoDataFrame(flows_impact_0, geometry='geometry')\n",
    "\n",
    "# 绘制空间可视化\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "gdf_impact_1.plot(ax=ax, color='red', legend=True, markersize='impact_flow', label='KmeansBymode_impact = 1')\n",
    "gdf_impact_0.plot(ax=ax, color='blue', legend=True, markersize='impact_flow', label='KmeansBymode_impact = 0')\n",
    "plt.title('Spatial Visualization of Flows')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend(title='KmeansBymode_impact', loc='upper right', bbox_to_anchor=(1.15, 1.0))\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GNN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Step 1: Create graph representation\n",
    "# Create an adjacency matrix (assuming 'toid' is used as node IDs)\n",
    "adjacency_matrix = pd.DataFrame(0, index=flows['toid'], columns=flows['toid'])\n",
    "for _, row in flows.iterrows():\n",
    "    adjacency_matrix.at[row['toid'], row['toid']] = 1\n",
    "\n",
    "# Create impact_flow tensor (assuming 'impact_flow' is already prepared as a PyTorch tensor)\n",
    "impact_flow_tensors = torch.tensor(flows['impact_flow'].values, dtype=torch.float)\n",
    "\n",
    "# Convert other node attributes to PyTorch tensors\n",
    "toid_tensors = torch.tensor(flows['toid'].values, dtype=torch.long)\n",
    "mode_tensors = torch.tensor(flows['mode'].values, dtype=torch.long)\n",
    "classification_tensors = torch.tensor(flows['classification'].values, dtype=torch.long)\n",
    "geometry_tensors = torch.tensor(flows['geometry'].values, dtype=torch.float)\n",
    "directiona_tensors = torch.tensor(flows['directiona'].values, dtype=torch.float)\n",
    "length_tensors = torch.tensor(flows['length'].values, dtype=torch.float)\n",
    "roadwidthm_tensors = torch.tensor(flows['roadwidthm'].values, dtype=torch.float)\n",
    "elevationg_tensors = torch.tensor(flows['elevationg'].values, dtype=torch.float)\n",
    "boundary_tensors = torch.tensor(flows['Boundary'].values, dtype=torch.long)\n",
    "\n",
    "# Concatenate all node attributes into a single tensor for each node\n",
    "x = torch.stack([impact_flow_tensors, toid_tensors, mode_tensors, classification_tensors, geometry_tensors,\n",
    "                 directiona_tensors, length_tensors, roadwidthm_tensors, elevationg_tensors, boundary_tensors], dim=1)\n",
    "\n",
    "# Create Data object for the graph\n",
    "data = Data(x=x, edge_index=adjacency_matrix.values.nonzero())\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define the GNN model\n",
    "class GNNModel(MessagePassing):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNNModel, self).__init__(aggr='add')  # Use 'add' aggregation for message passing\n",
    "        self.lin1 = nn.Linear(in_channels, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.propagate(edge_index, x=x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        return x\n",
    "\n",
    "    def message(self, x_j):\n",
    "        return x_j\n",
    "\n",
    "\n",
    "# Step 4: Model training\n",
    "\n",
    "# Prepare the DataLoader for training the GNN model\n",
    "loader = DataLoader([data], batch_size=1)\n",
    "\n",
    "# Create the GNN model instance\n",
    "model = GNNModel(in_channels=data.num_node_features, hidden_channels=64, out_channels=32)\n",
    "\n",
    "# Set optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Train the GNN model\n",
    "model.train()\n",
    "for data in loader:\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = loss_function(out, data.x)  # Assuming impact_flow is used as node feature\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Step 5: Clustering and identifying roads with significant changes\n",
    "\n",
    "\n",
    "\n",
    "# Assuming the model is already trained\n",
    "\n",
    "# Get node embeddings from the trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    node_embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "# Use KMeans clustering on the node embeddings\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(node_embeddings.numpy())\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "flows['cluster'] = clusters\n",
    "\n",
    "# Now, you can analyze the clusters and identify the roads with significant impact flow changes\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-02T04:41:40.117015Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
