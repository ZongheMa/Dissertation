{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import IPython.display as display\n",
    "import copy\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def merge_csv_files(directory):\n",
    "    # Get a list of all the csv files\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "    # Initialize an empty list to hold dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through csv files, read each into a dataframe, and append to the list\n",
    "    for file in csv_files:\n",
    "        # Extract date from filename, assuming the date is in format 'traffic_flow_YYYY_MM_DD'\n",
    "        date_str = file.split('.')[0].split('_')[-3:]  # This gives ['YYYY', 'MM', 'DD']\n",
    "        date = datetime.strptime('_'.join(date_str), '%Y_%m_%d').date()\n",
    "\n",
    "        df = pd.read_csv(os.path.join(directory, file))\n",
    "\n",
    "        # Add date as a new column\n",
    "        df['date'] = date.strftime('%m/%d/%y')\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all dataframes in the list into one dataframe\n",
    "    merged_df = pd.concat(dfs, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # Return the merged dataframe\n",
    "    return merged_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "traffic_flows = merge_csv_files(\n",
    "    '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/[XH]Traffic flow')\n",
    "road_network = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/[XH]road_network/road_network.shp'\n",
    "\n",
    "# clean the traffic flow data\n",
    "traffic_flows = traffic_flows.drop_duplicates(['toid', 'date'])\n",
    "traffic_flows = traffic_flows.groupby(['toid', 'date']).agg(\n",
    "    {'bus': 'sum', 'car': 'sum', 'cycle': 'sum', 'walks': 'sum', 'stationary': 'sum'}).reset_index()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lsoa = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/London administrative boundaries/london_LSOA/london_LSOA.shp'\n",
    "road_network = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/[XH]road_network/road_network.shp'\n",
    "inoutter = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/London administrative boundaries/lp-consultation-oct-2009-inner-outer-london-shp/lp-consultation-oct-2009-inner-outer-london.shp'\n",
    "tube_line = 'https://raw.githubusercontent.com/oobrien/vis/master/tubecreature/data/tfl_lines.json'\n",
    "tube_station = 'https://raw.githubusercontent.com/oobrien/vis/master/tubecreature/data/tfl_stations.json'\n",
    "\n",
    "inoutter = gpd.read_file(inoutter)\n",
    "inoutter.to_crs(epsg=27700, inplace=True)\n",
    "\n",
    "# tube_station = gpd.read_file(tube_station)\n",
    "# tube_station.to_crs(epsg=27700, inplace=True)\n",
    "# tube_station = gpd.sjoin(tube_station, inoutter, op='within')\n",
    "\n",
    "tube_line = gpd.read_file(tube_line)\n",
    "tube_line.to_crs(epsg=27700, inplace=True)\n",
    "tube_line = gpd.sjoin(tube_line, inoutter, op='within')\n",
    "\n",
    "lsoa = gpd.read_file(lsoa, crs={'init': 'epsg:27700'})\n",
    "road_network = gpd.read_file(road_network, crs={'init': 'epsg:27700'})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# clean the traffic flow data\n",
    "traffic_flows = traffic_flows.drop_duplicates(['toid', 'date'])\n",
    "traffic_flows = traffic_flows.groupby(['toid', 'date']).agg(\n",
    "    {'bus': 'sum', 'car': 'sum', 'cycle': 'sum', 'walks': 'sum', 'stationary': 'sum'}).reset_index()\n",
    "traffic_flows['total'] = traffic_flows['bus'] + traffic_flows['car'] + traffic_flows['cycle'] + traffic_flows[\n",
    "    'walks'] + traffic_flows['stationary']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flows = pd.merge(\n",
    "    road_network[['toid', 'roadclassi', 'geometry', 'directiona', 'length', 'roadwidthm', 'elevationg'\n",
    "                  ]],\n",
    "    traffic_flows, left_on='toid', right_on='toid', how='right')\n",
    "\n",
    "flows['classification'] = flows['roadclassi'].replace(\n",
    "    {'Unknown': 'Local Road', 'Not Classified': 'Local Road', 'Unclassified': 'Local Road',\n",
    "     'Classified Unnumbered': 'Local Road'})\n",
    "\n",
    "flows.drop(columns=['roadclassi'], inplace=True)\n",
    "\n",
    "stage_date = ['03/01/22', '02/22/22', '03/08/22']\n",
    "flows = flows.loc[flows['date'].isin(stage_date)]\n",
    "\n",
    "# label the regional level\n",
    "flows = gpd.sjoin(flows, inoutter, how='inner', predicate='within')\n",
    "flows = flows.drop(columns=['index_right', 'Source', 'Area_Ha', 'Shape_Leng', 'Shape_Area'])\n",
    "flows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# convert the dataframe\n",
    "flows = pd.melt(flows,\n",
    "                id_vars=['toid', 'classification', 'geometry', 'directiona', 'length', 'roadwidthm', 'elevationg',\n",
    "                         'date', 'Boundary'], var_name='mode', value_name='flow')\n",
    "\n",
    "flows = pd.pivot_table(flows,\n",
    "                       index=['toid', 'classification', 'geometry', 'directiona', 'length', 'roadwidthm', 'elevationg',\n",
    "                              'date', 'Boundary', 'mode'], columns='date', values='flow', aggfunc='first').reset_index()\n",
    "\n",
    "flows.drop(columns=['date'], inplace=True)\n",
    "flows = flows.groupby(\n",
    "    ['toid', 'mode', 'classification', 'geometry', 'directiona', 'length', 'roadwidthm', 'elevationg',\n",
    "     'Boundary'], as_index=False).agg(\n",
    "    {'03/01/22': 'first', '02/22/22': 'first', '03/08/22': 'first'})\n",
    "\n",
    "# calculate the impact and recovery flows for one strike\n",
    "flows['impact_flow'] = flows['03/01/22'] - flows['02/22/22']\n",
    "flows['recovery_flow'] = flows['03/08/22'] - flows['03/01/22']\n",
    "\n",
    "All = flows.copy()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Take city of  london as an example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the city of london LA\n",
    "LA = gpd.read_file(\n",
    "    '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/London administrative boundaries/LA/LA .shp',\n",
    "    crs={'init': 'epsg:27700'})\n",
    "CoL = LA.loc[LA['LAD22NM'] == 'City of London']\n",
    "\n",
    "# spatial clip of the flows data\n",
    "flows = gpd.GeoDataFrame(flows, geometry='geometry')\n",
    "CoL_flows = gpd.clip(flows, CoL)\n",
    "\n",
    "flows = CoL_flows.copy().reset_index(drop=True)\n",
    "\n",
    "flows.plot()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flows"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graph"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flows = flows[flows['mode'] == 'total']\n",
    "flows.reset_index(drop=True)\n",
    "\n",
    "# Create an empty graph\n",
    "graph = nx.Graph()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creat a blank graph\n",
    "graph = nx.Graph()\n",
    "\n",
    "# iterate over the rows of the flows DataFrame\n",
    "for _, row in flows.iterrows():\n",
    "    geometry = row['geometry']\n",
    "    baseline_1 = row['02/22/22']\n",
    "    strike_1 = row['03/01/22']\n",
    "    recovery_1 = row['03/08/22']\n",
    "    impact_flow = row['impact_flow']\n",
    "    recovery_flow = row['recovery_flow']\n",
    "    direction = row['directiona']\n",
    "\n",
    "    # break the MultiLineString geometry into its constituent LineStrings\n",
    "    if geometry.geom_type == 'MultiLineString':\n",
    "        for line_string in geometry.geoms:  # iterate over each LineString\n",
    "            from_node = line_string.coords[0]\n",
    "            to_node = line_string.coords[-1]\n",
    "\n",
    "            # Add nodes to the graph\n",
    "            graph.add_node(from_node, pos=from_node)  # Use 'from_node' as the node position\n",
    "            graph.add_node(to_node, pos=to_node)  # Use 'to_node' as the node position\n",
    "            # Add edges to the graph based on the direction\n",
    "            if direction == 'bothDirections':\n",
    "                # If the road is bidirectional, flows are split equally in both directions\n",
    "                graph.add_edge(from_node, to_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                               recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                               toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "                graph.add_edge(to_node,\n",
    "                               from_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2, recovery_1=recovery_1 / 2,\n",
    "                               impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                               toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "            elif direction == 'inOppositeDirection':\n",
    "                # If the road is in the opposite direction, flows are from the ending point to the starting point\n",
    "                graph.add_edge(to_node,\n",
    "                               from_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2, recovery_1=recovery_1 / 2,\n",
    "                               impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                               toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "            elif direction == 'inDirection':\n",
    "                # If the road is in the same direction, flows are from the starting point to the ending point\n",
    "                graph.add_edge(from_node, to_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                               recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                               toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "    else:\n",
    "        from_node = geometry.coords[0]\n",
    "        to_node = geometry.coords[-1]\n",
    "\n",
    "        # Add nodes to the graph\n",
    "        graph.add_node(from_node, pos=from_node)  # Use 'from_node' as the node position\n",
    "        graph.add_node(to_node, pos=to_node)  # Use 'to_node' as the node position\n",
    "        # Add edges to the graph based on the direction\n",
    "        if direction == 'bothDirections':\n",
    "            # If the road is bidirectional, flows are split equally in both directions\n",
    "            graph.add_edge(from_node, to_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                           recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                           toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "            graph.add_edge(to_node, from_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                           recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                           toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "        elif direction == 'inOppositeDirection':\n",
    "            # If the road is in the opposite direction, flows are from the ending point to the starting point\n",
    "            graph.add_edge(to_node, from_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                           recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                           toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "        elif direction == 'inDirection':\n",
    "            # If the road is in the same direction, flows are from the starting point to the ending point\n",
    "            graph.add_edge(from_node, to_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                           recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                           toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "\n",
    "def graph_visualization(graph, weight=None, cmap='Greens'):\n",
    "    # obtain all the edges in the graph\n",
    "    edges_list = list(graph.edges())\n",
    "\n",
    "    # make sure the graph is not empty\n",
    "    if edges_list:\n",
    "        # obtain the start and end coordinates of the first edge\n",
    "        u, v = edges_list[0]\n",
    "\n",
    "        # obtain the edge attribute of the first edge\n",
    "        edge_attr = graph[u][v]\n",
    "\n",
    "        print(f\"The first edge: ({u}, {v})\")\n",
    "        print(\"Attribute: \", edge_attr)\n",
    "    else:\n",
    "        print(\"No edge in the graph.\")\n",
    "\n",
    "    # get the node positions\n",
    "    node_positions = nx.get_node_attributes(graph, 'pos')\n",
    "\n",
    "    # get the edge weights\n",
    "    edge_weight = nx.get_edge_attributes(graph, weight)\n",
    "\n",
    "    # normalize the edge weights between 0 and 1\n",
    "    weight_values = list(edge_weight.values())\n",
    "    norm = Normalize(vmin=min(weight_values), vmax=max(weight_values))\n",
    "    norm_weight = {edge: norm(weight) for edge, weight in edge_weight.items()}\n",
    "\n",
    "    # create a colormap\n",
    "    cmap_object = plt.get_cmap(cmap)\n",
    "    mappable = ScalarMappable(norm=norm, cmap=cmap_object)\n",
    "    mappable.set_array([])\n",
    "\n",
    "    # plot the graph\n",
    "    fig, ax = plt.subplots()\n",
    "    nx.draw_networkx_edges(graph, pos=node_positions, edge_color='gray')\n",
    "    nx.draw_networkx_edges(graph, pos=node_positions,\n",
    "                           edge_color=[mappable.to_rgba(norm_weight[edge]) for edge in edges_list])\n",
    "\n",
    "    # add a colorbar\n",
    "    cbar = plt.colorbar(mappable, ax=ax, orientation='horizontal', pad=0.01)\n",
    "    cbar.set_label(weight)\n",
    "\n",
    "    plt.title(f'Graph Representation of the Road Network', size=10)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_visualization(graph, weight='impact_flow')\n",
    "graph_visualization(graph, weight='recovery_flow')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Calculate the structure-based indicators for roads and system\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate the number of nodes\n",
    "nodes_number = graph.number_of_nodes()\n",
    "\n",
    "# calculate the number of edges\n",
    "links_number = graph.number_of_edges()\n",
    "\n",
    "# calculate the total link weight\n",
    "total_link_weight = sum([data['impact_flow'] for u, v, data in graph.edges(data=True)])\n",
    "\n",
    "mean_link_weight = total_link_weight / links_number\n",
    "\n",
    "# calculate the coefficient of variation of node degree\n",
    "node_degrees = dict(graph.degree())\n",
    "mean_node_degree = np.mean(list(node_degrees.values()))\n",
    "std_node_degree = np.std(list(node_degrees.values()))\n",
    "node_degree_cv = (std_node_degree / mean_node_degree) * 100\n",
    "\n",
    "# calculate the coefficient of variation of edge weight\n",
    "edge_weights = nx.get_edge_attributes(graph, 'impact_flow').values()\n",
    "mean_edge_weight = np.mean(list(edge_weights))\n",
    "std_edge_weight = np.std(list(edge_weights))\n",
    "edge_weight_cv = (std_edge_weight / mean_edge_weight) * 100\n",
    "\n",
    "# calculate the network connectivity and score for graph\n",
    "network_connectivity = nx.is_connected(graph)\n",
    "connectivity_score = 2 * links_number / (nodes_number * nodes_number)\n",
    "\n",
    "# calculate the average clustering coefficient for graph\n",
    "avg_clustering_coefficient = nx.average_clustering(graph)\n",
    "\n",
    "# calculate the transitivity for graph\n",
    "transitivity = nx.transitivity(graph)\n",
    "\n",
    "# calculate the assortativity for graph\n",
    "assortativity = nx.degree_assortativity_coefficient(graph)\n",
    "\n",
    "# calculate indicators as attributes for each road\n",
    "clustering_coefficients = nx.clustering(graph)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(graph)\n",
    "\n",
    "# calculate the node degrees\n",
    "node_degrees = graph.degree()\n",
    "\n",
    "# calculate the average degree for graph\n",
    "average_degree = sum(dict(node_degrees).values()) / len(node_degrees)\n",
    "\n",
    "# calculate the betweenness_centrality for each road\n",
    "betweenness_centrality = nx.betweenness_centrality(graph)\n",
    "\n",
    "# Add the calculated indicators as attributes for each road\n",
    "for u, v in graph.edges():\n",
    "    edge_clustering_coefficient = (clustering_coefficients[u] + clustering_coefficients[v]) / 2\n",
    "    graph[u][v]['clustering_coefficient'] = edge_clustering_coefficient\n",
    "\n",
    "    graph[u][v]['degree'] = (node_degrees[u] + node_degrees[v]) / 2\n",
    "    graph[u][v]['betweenness'] = (betweenness_centrality[u] + betweenness_centrality[v]) / 2\n",
    "\n",
    "    edge_eigenvector_centrality = (eigenvector_centrality[u] + eigenvector_centrality[v]) / 2\n",
    "    graph[u][v]['eigenvector_centrality'] = edge_eigenvector_centrality\n",
    "\n",
    "# Print the calculated indicators\n",
    "print(\"Total Nodes Number:\", nodes_number)\n",
    "print(\"Total Links Number:\", links_number)\n",
    "print(\"Total Flows:\", round(total_link_weight))\n",
    "print(\"Mean Link Flow:\", mean_link_weight)\n",
    "print(\"Node Degree Coefficient of Variation:\", node_degree_cv)\n",
    "print(\"Edge Weight Coefficient of Variation:\", edge_weight_cv)\n",
    "print(\"Connectivity Score:\", connectivity_score)\n",
    "print(\"Network Connectivity:\", network_connectivity)\n",
    "print(\"Transitivity:\", transitivity)\n",
    "print(\"Assortativity:\", assortativity)\n",
    "print(\"Average Clustering Coefficient:\", avg_clustering_coefficient)\n",
    "print(\"Average Degree:\", average_degree)\n",
    "print(\"Average Betweenness Centrality:\", sum(betweenness_centrality.values()) / len(betweenness_centrality))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "impact_flow复原性和脆弱性分析：\n",
    "\n",
    "**复原性分析：**\n",
    "- **Total Nodes Number: 1104** 和 **Total Links Number: 1227** 表示路网的规模。规模较大可能需要更多资源来维护，但在复原性方面，如果网络中有多个节点和连接，也可能有更多的备选路径。\n",
    "\n",
    "- **Total Flows: -36378** 和 **Mean Link Flow: -29.65** 表示流量变化差值和平均链路流量。负数的流量变化差值可能表示流量减少。这可能会导致某些连接拥塞或资源分配不当，对于复原性可能带来一些挑战。\n",
    "\n",
    "- **Node Degree Coefficient of Variation: 43.65** 和 **Edge Weight Coefficient of Variation: -449.42** 表示节点度数和边权重的变异程度。负的边权重变异系数可能反映了权重值的分布不均匀。较高的节点度数变异系数可能意味着一些节点的连接更为密集，但是这种变异可能影响网络的复原性。\n",
    "\n",
    "- **Transitivity: 0.064** 表示节点的传递性。传递性的增加可能有助于信息或流量在网络中的快速传播，提高复原性。\n",
    "\n",
    "- **Assortativity: 0.032** 表示网络的度同配性。这种度同配性可能有助于信息在网络中传播，但是取值较低，作用可能不太明显。\n",
    "\n",
    "**脆弱性分析：**\n",
    "- **Connectivity Score: 0.00201** 和 **Network Connectivity: False** 表示连接性分数较低和网络不是全连通的。这可能意味着网络中可能存在一些孤立的区域，当这些区域受到影响时，整个网络的连通性会受到影响。\n",
    "\n",
    "- **Average Clustering Coefficient: 0.036** 表示平均聚类系数。较低的聚类系数可能意味着节点之间关联性不高，可能导致信息难以在网络中传播。\n",
    "\n",
    "- **Average Betweenness Centrality: 0.014** 表示平均介数中心性，较低的介数中心性可能表示网络中的信息流和影响受限。\n",
    "\n",
    "综合分析，以流量变化差值为权重的城市伦敦路网结构可能具有一定的复原性挑战和脆弱性。负数的流量变化可能表明某些区域可能存在拥塞或资源分配不足。节点度数和边权重的变异可能意味着网络中存在一些密集连接的节点和不均匀的权重分布，这可能会影响网络的鲁棒性。然而，一些正向因素如传递性、度同配性等可能有助于信息传播和一些程度的复原。在增强路网的复原性和脆弱性方面，可能需要考虑网络的连通性、资源分配、信息传播等因素。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Strike_1复原性分析：**\n",
    "- **Total Nodes Number: 1104** 和 **Total Links Number: 1227** 表示路网的规模。路网的大小可能对于复原性有影响，大规模路网可能需要更多的资源来维持。\n",
    "\n",
    "- **Node Degree Coefficient of Variation: 43.65** 和 **Edge Weight Coefficient of Variation: 115.01** 表示节点度数和边权重的变异程度。较高的变异系数可能意味着部分节点度数和边权重变化较大，这可能增加了复原过程的难度，因为某些节点或边更容易受到影响。\n",
    "\n",
    "- **Transitivity: 0.064** 表示节点的传递性。传递性的增加可能有助于信息或流量在网络中的快速传播，这对复原性可能有积极影响。\n",
    "\n",
    "- **Assortativity: 0.032** 表示网络的度同配性，即高度连接的节点倾向于连接到其他高度连接的节点。这种连接模式可能有助于信息在网络中传播，从而提高复原性。\n",
    "\n",
    "**脆弱性分析：**\n",
    "- **Network Connectivity: False** 表示网络不是全连通的，可能存在多个网络组件。这可能导致某些区域或网络组件受到故障影响时，整个网络的连通性受到影响。\n",
    "\n",
    "- **Connectivity Score: 0.00201** 表示网络的连接性分数较低。低连接性分数可能意味着网络中的连接较弱，复原能力较弱。\n",
    "\n",
    "- **Average Clustering Coefficient: 0.036** 表示平均聚类系数。较低的聚类系数可能表示节点之间的关联性不高，网络的鲁棒性可能较弱。\n",
    "\n",
    "- **Average Betweenness Centrality: 0.014** 表示平均介数中心性，介数中心性衡量节点在网络中作为桥梁的程度。较低的介数中心性可能意味着网络中的信息流和影响可能受限。\n",
    "\n",
    "综合分析，CoL路网结构可能具有一定的复原性挑战和脆弱性。较高的节点度数和边权重变异系数、低连接性分数、不高的平均聚类系数和介数中心性可能意味着在面对故障、拥塞、攻击等情况时，网络的复原能力可能受到一定影响。但是，度同配性、传递性等正向因素可能有助于一些程度上的复原和信息传播。为了增强路网的复原性和脆弱性，可能需要关注网络连接性、鲁棒性、信息传播能力等方面的改进。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_visualization(graph, weight='betweenness')\n",
    "graph_visualization(graph, weight='clustering_coefficient')\n",
    "graph_visualization(graph, weight='eigenvector_centrality')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network DBSCAN Clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 将边的权重属性值作为数据点\n",
    "edges_list = list(graph.edges())\n",
    "data_points = np.array([graph[u][v]['impact_flow'] for u, v in edges_list]).reshape(-1, 1)\n",
    "# 创建模型\n",
    "dbscan = DBSCAN()\n",
    "\n",
    "# 定义要搜索的参数范围\n",
    "param_grid = {'eps': [1, 1.5, 2, 2.5, 3, 4, 5, 6, 7, 8, 9, 10], 'min_samples': [1, 2, 3, 4, 5, 6]}\n",
    "\n",
    "best_score = -1\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "\n",
    "# 在数据上执行交叉验证\n",
    "for eps in param_grid['eps']:\n",
    "    for min_samples in param_grid['min_samples']:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(data_points)\n",
    "        if len(set(labels)) > 1:  # 忽略只有一个簇的情况\n",
    "            score = silhouette_score(data_points, labels)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_eps = eps\n",
    "                best_min_samples = min_samples\n",
    "\n",
    "print(\"Best EPS:\", best_eps)\n",
    "print(\"Best Min Samples:\", best_min_samples)\n",
    "\n",
    "# 使用最佳参数进行DBSCAN聚类\n",
    "best_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "best_labels = best_dbscan.fit_predict(data_points)\n",
    "\n",
    "# 将聚类结果应用于图\n",
    "for i, (u, v) in enumerate(edges_list):\n",
    "    graph[u][v]['cluster_DB'] = best_labels[i]\n",
    "\n",
    "# 打印聚类的唯一值\n",
    "unique_clusters = set(best_labels)\n",
    "print(\"Unique Cluster Values:\", unique_clusters)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 创建颜色映射\n",
    "cmap = plt.get_cmap('tab20', len(unique_clusters))\n",
    "\n",
    "# 绘制图形\n",
    "fig, ax = plt.subplots()\n",
    "pos_cluster = nx.fruchterman_reingold_layout(graph)\n",
    "\n",
    "for u, v, attr in graph.edges(data=True):\n",
    "    cluster = attr['cluster_DB']\n",
    "    if cluster == -1:\n",
    "        edge_color = 'lightgrey'  # 将 -1 标签的边设置为浅灰色\n",
    "    else:\n",
    "        edge_color = cmap(cluster)\n",
    "    nx.draw_networkx_edges(graph, pos_cluster, edgelist=[(u, v)], width=1, edge_color=edge_color)\n",
    "\n",
    "# 创建不连续的分类点图例\n",
    "unique_labels = np.unique(best_labels)\n",
    "handles = []\n",
    "for label in unique_labels:\n",
    "    if label == -1:  # 处理 -1 标签\n",
    "        handle = plt.Line2D([], [], color='lightgrey', marker='o', markersize=10, label='Noise')\n",
    "    else:\n",
    "        color = cmap(label)\n",
    "        handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "    handles.append(handle)\n",
    "\n",
    "# 添加图例\n",
    "ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title(\"DBSCAN Clustering of Graph Edges\")\n",
    "plt.show()\n",
    "\n",
    "node_positions = nx.get_node_attributes(graph, 'pos')\n",
    "nx.draw_networkx_edges(graph, node_positions, width=1, edge_color=cmap(best_labels))\n",
    "plt.title(\"DBSCAN Clustering in network\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network K-Means Clustering\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### By Degree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 从图中获取节点的度作为特征\n",
    "node_features = np.array([graph.degree(node) for node in graph.nodes()]).reshape(-1, 1)\n",
    "\n",
    "# 使用K均值算法进行聚类\n",
    "k = 7  # 聚类数量\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "cluster_labels = kmeans.fit_predict(node_features)\n",
    "\n",
    "# 将聚类结果应用于图\n",
    "for i, node in enumerate(graph.nodes()):\n",
    "    graph.nodes[node]['cluster_k'] = cluster_labels[i]\n",
    "\n",
    "# 初始化节点的累计flow和度的字典\n",
    "node_weight = {node: 0 for node in graph.nodes()}\n",
    "node_degrees = {node: 0 for node in graph.nodes()}\n",
    "\n",
    "# 遍历边，累计flow和度\n",
    "for u, v, attr in graph.edges(data=True):\n",
    "    weight = attr['impact_flow']\n",
    "    node_weight[u] += weight\n",
    "    node_weight[v] += weight\n",
    "    node_degrees[u] += 1\n",
    "    node_degrees[v] += 1\n",
    "\n",
    "# 计算每个节点的特征（平均flow）\n",
    "node_features = np.array(\n",
    "    [node_weight[node] / node_degrees[node] if node_degrees[node] > 0 else 0 for node in graph.nodes()]).reshape(-1, 1)\n",
    "\n",
    "# 创建颜色映射\n",
    "cmap = plt.get_cmap('tab20', k)\n",
    "\n",
    "# 绘制图形\n",
    "fig, ax = plt.subplots()\n",
    "pos = nx.fruchterman_reingold_layout(graph)\n",
    "\n",
    "for u, v, attr in graph.edges(data=True):\n",
    "    u_cluster = graph.nodes[u]['cluster_k']\n",
    "    v_cluster = graph.nodes[v]['cluster_k']\n",
    "\n",
    "    if u_cluster == v_cluster:\n",
    "        cluster = u_cluster\n",
    "    else:\n",
    "        cluster = -1  # 表示不同的聚类\n",
    "    if cluster == -1:\n",
    "        edge_color = 'lightgrey'  # 将不同聚类的边设置为浅灰色\n",
    "    else:\n",
    "        edge_color = cmap(cluster)\n",
    "    nx.draw_networkx_edges(graph, pos_cluster, edgelist=[(u, v)], width=1, edge_color=edge_color)\n",
    "\n",
    "# 创建不连续的分类点图例\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "handles = []\n",
    "for label in unique_labels:\n",
    "    if label == -1:  # 处理 -1 标签\n",
    "        handle = plt.Line2D([], [], color='lightgrey', marker='o', markersize=10, label='Noise')\n",
    "    else:\n",
    "        color = cmap(label)\n",
    "        handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "    handles.append(handle)\n",
    "\n",
    "# 添加图例\n",
    "ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title(\"KMeans Clustering of Graph Nodes\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the clusters in the network\n",
    "node_positions = nx.get_node_attributes(graph, 'pos')\n",
    "nx.draw_networkx_edges(graph, node_positions, width=1, edge_color=cmap(cluster_labels))\n",
    "# 创建不连续的分类点图例\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "handles = []\n",
    "for label in unique_labels:\n",
    "    if label == -1:  # 处理 -1 标签\n",
    "        handle = plt.Line2D([], [], color='lightgrey', marker='o', markersize=10, label='Noise')\n",
    "    else:\n",
    "        color = cmap(label)\n",
    "        handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "    handles.append(handle)\n",
    "\n",
    "# 添加图例\n",
    "ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.title(\"KMeans Clustering in network\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### By Flow changes / Clustering Coefficient"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 获取所有边的列表\n",
    "edges_list = list(graph.edges())\n",
    "\n",
    "# 按照flows聚类\n",
    "# 初始化特征矩阵\n",
    "num_edges = len(edges_list)\n",
    "feature_matrix = np.zeros((num_edges, 1))  # 1列，代表 \"flow\" 属性值\n",
    "\n",
    "# 填充特征矩阵\n",
    "for i, (u, v) in enumerate(edges_list):\n",
    "    flow = graph[u][v]['impact_flow']\n",
    "    feature_matrix[i, 0] = flow\n",
    "\n",
    "# #按照Clustering Coefficient聚类\n",
    "# # 初始化特征矩阵\n",
    "# num_edges = len(edges_list)\n",
    "# num_nodes = len(graph.nodes())\n",
    "# feature_matrix = np.zeros((num_edges, num_nodes))\n",
    "# \n",
    "# # 填充特征矩阵\n",
    "# for i, (u, v) in enumerate(edges_list):\n",
    "#     u_idx = list(graph.nodes()).index(u)  # 获取节点 u 的整数索引\n",
    "#     v_idx = list(graph.nodes()).index(v)  # 获取节点 v 的整数索引\n",
    "#     u_clustering = nx.clustering(graph)[u]\n",
    "#     v_clustering = nx.clustering(graph)[v]\n",
    "#     feature_matrix[i, u_idx] = u_clustering\n",
    "#     feature_matrix[i, v_idx] = v_clustering\n",
    "\n",
    "# 使用K均值算法进行聚类\n",
    "k = 5  # 聚类数量\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "# 将聚类结果应用于图\n",
    "for i, (u, v) in enumerate(edges_list):\n",
    "    graph[u][v]['cluster_k'] = cluster_labels[i]\n",
    "\n",
    "# 创建颜色映射\n",
    "cmap = plt.get_cmap('tab20', k)\n",
    "\n",
    "# 绘制图形\n",
    "fig, ax = plt.subplots()\n",
    "pos = nx.spring_layout(graph)\n",
    "\n",
    "for u, v, attr in graph.edges(data=True):\n",
    "    cluster = attr['cluster_k']\n",
    "    edge_color = cmap(cluster)\n",
    "    nx.draw_networkx_edges(graph, pos, edgelist=[(u, v)], width=1, edge_color=edge_color)\n",
    "\n",
    "# 创建不连续的分类点图例\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "handles = []\n",
    "for label in unique_labels:\n",
    "    color = cmap(label)\n",
    "    handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "    handles.append(handle)\n",
    "\n",
    "ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.axis('off')\n",
    "plt.title(\"Edge Flow Changes-based Clustering\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the clusters in the network\n",
    "node_positions = nx.get_node_attributes(graph, 'pos')\n",
    "nx.draw_networkx_edges(graph, node_positions, width=1, edge_color=cmap(cluster_labels))\n",
    "# 创建不连续的分类点图例\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "handles = []\n",
    "for label in unique_labels:\n",
    "    if label == -1:  # 处理 -1 标签\n",
    "        handle = plt.Line2D([], [], color='lightgrey', marker='o', markersize=10, label='Noise')\n",
    "    else:\n",
    "        color = cmap(label)\n",
    "        handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "    handles.append(handle)\n",
    "\n",
    "# 添加图例\n",
    "ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.title(\"KMeans Clustering in network\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export the graph and update the indicators to All dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "update = All.copy()\n",
    "\n",
    "# 创建一个空的 Pandas DataFrame\n",
    "columns = ['toid', 'clustering_coefficient', 'degree', 'betweenness',\n",
    "           'eigenvector_centrality', 'cluster_DB', 'cluster_k']\n",
    "data = []\n",
    "\n",
    "# 将图的每条边导出到 DataFrame\n",
    "for _, _, edge_data in graph.edges(data=True):\n",
    "    row_data = {'toid': edge_data['toid'],\n",
    "                # 'strike_1': edge_data['strike_1'], 'baseline_1': edge_data['baseline_1'],\n",
    "                # 'recovery_1': edge_data['recovery_1'],\n",
    "                # 'impact_flow': edge_data['impact_flow'], 'recovery_flow': edge_data['recovery_flow'],\n",
    "                # 'classification': edge_data['classification'], 'geometry': edge_data['geometry'],\n",
    "                'clustering_coefficient': edge_data['clustering_coefficient'], 'degree': edge_data['degree'],\n",
    "                'betweenness': edge_data['betweenness'], 'eigenvector_centrality': edge_data['eigenvector_centrality'],\n",
    "                'cluster_DB': edge_data['cluster_DB'], 'cluster_k': edge_data['cluster_k']}\n",
    "    data.append(row_data)\n",
    "\n",
    "# 创建 DataFrame\n",
    "graph_df = pd.DataFrame(data, columns=columns)\n",
    "graph_df['mode']='total'\n",
    "\n",
    "update['toid'] = update['toid'].astype(str)\n",
    "\n",
    "update = pd.merge(update, graph_df, on=['toid','mode'], how='left')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# need to merge all updates to All dataframe\n",
    "update"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flow changes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# 示例数据\n",
    "source = [0, 0, 1, 1, 2]\n",
    "target = [2, 3, 3, 4, 4]\n",
    "value = [120, 20, 50, 30, 40]\n",
    "labels = ['Input 1', 'Input 2', 'Output 1', 'Output 2', 'Output 3']\n",
    "\n",
    "# 创建Sankey图\n",
    "fig = go.Figure(go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color='black', width=0.5),\n",
    "        label=labels\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram\", font_size=10)\n",
    "fig.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
