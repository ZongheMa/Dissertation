{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:33:32.666362Z",
     "start_time": "2023-08-13T20:33:32.407111Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/geospatial/lib/python3.8/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/var/folders/1k/27mkp8bj3ps60c3nmr7rbqzh0000gn/T/ipykernel_12319/1895528008.py:1: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import IPython.display as display\n",
    "import copy\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def merge_csv_files(directory):\n",
    "    # Get a list of all the csv files\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "    # Initialize an empty list to hold dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through csv files, read each into a dataframe, and append to the list\n",
    "    for file in csv_files:\n",
    "        # Extract date from filename, assuming the date is in format 'traffic_flow_YYYY_MM_DD'\n",
    "        date_str = file.split('.')[0].split('_')[-3:]  # This gives ['YYYY', 'MM', 'DD']\n",
    "        date = datetime.strptime('_'.join(date_str), '%Y_%m_%d').date()\n",
    "\n",
    "        df = pd.read_csv(os.path.join(directory, file))\n",
    "\n",
    "        # Add date as a new column\n",
    "        df['date'] = date.strftime('%m/%d/%y')\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all dataframes in the list into one dataframe\n",
    "    merged_df = pd.concat(dfs, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # Return the merged dataframe\n",
    "    return merged_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:33:33.522995Z",
     "start_time": "2023-08-13T20:33:33.510376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "traffic_flows = merge_csv_files(\n",
    "    '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/[XH]Traffic flow')\n",
    "road_network = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/[XH]road_network/road_network.shp'\n",
    "\n",
    "# clean the traffic flow data\n",
    "traffic_flows = traffic_flows.drop_duplicates(['toid', 'date'])\n",
    "traffic_flows = traffic_flows.groupby(['toid', 'date']).agg(\n",
    "    {'bus': 'sum', 'car': 'sum', 'cycle': 'sum', 'walks': 'sum', 'stationary': 'sum'}).reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:33:50.844835Z",
     "start_time": "2023-08-13T20:33:34.841852Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "lsoa = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/London administrative boundaries/london_LSOA/london_LSOA.shp'\n",
    "\n",
    "inoutter = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/London administrative boundaries/lp-consultation-oct-2009-inner-outer-london-shp/lp-consultation-oct-2009-inner-outer-london.shp'\n",
    "# tube_line = 'https://raw.githubusercontent.com/oobrien/vis/master/tubecreature/data/tfl_lines.json'\n",
    "# tube_station = 'https://raw.githubusercontent.com/oobrien/vis/master/tubecreature/data/tfl_stations.json'\n",
    "\n",
    "inoutter = gpd.read_file(inoutter)\n",
    "inoutter.to_crs(epsg=27700, inplace=True)\n",
    "\n",
    "# tube_station = gpd.read_file(tube_station)\n",
    "# tube_station.to_crs(epsg=27700, inplace=True)\n",
    "# tube_station = gpd.sjoin(tube_station, inoutter, op='within')\n",
    "\n",
    "# tube_line = gpd.read_file(tube_line)\n",
    "# tube_line.to_crs(epsg=27700, inplace=True)\n",
    "# tube_line = gpd.sjoin(tube_line, inoutter, op='within')\n",
    "\n",
    "lsoa = gpd.read_file(lsoa, crs={'init': 'epsg:27700'})\n",
    "road_network = gpd.read_file(road_network, crs={'init': 'epsg:27700'})\n",
    "road_network.rename(columns={'NAME': 'boroughs'}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:34:23.219607Z",
     "start_time": "2023-08-13T20:33:50.848417Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# clean the traffic flow data\n",
    "traffic_flows = traffic_flows.drop_duplicates(['toid', 'date'])\n",
    "traffic_flows = traffic_flows.groupby(['toid', 'date']).agg(\n",
    "    {'bus': 'sum', 'car': 'sum', 'cycle': 'sum', 'walks': 'sum', 'stationary': 'sum'}).reset_index()\n",
    "traffic_flows['total'] = traffic_flows['bus'] + traffic_flows['car'] + traffic_flows['cycle'] + traffic_flows[\n",
    "    'walks'] + traffic_flows['stationary']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:34:27.247780Z",
     "start_time": "2023-08-13T20:34:23.326202Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "flows = pd.merge(\n",
    "    road_network[\n",
    "        ['toid', 'roadclassi', 'geometry', 'cycle_lane', 'bus_lane', 'boroughs']],\n",
    "    traffic_flows, left_on='toid', right_on='toid', how='left')\n",
    "flows.set_geometry('geometry', inplace=True)\n",
    "\n",
    "flows['classification'] = flows['roadclassi'].replace(\n",
    "    {'Unknown': 'Local Road', 'Not Classified': 'Local Road', 'Unclassified': 'Local Road',\n",
    "     'Classified Unnumbered': 'Local Road', 'A Road': 'Strategic Road', 'B Road': 'Strategic Road'})\n",
    "flows.drop(columns=['roadclassi'], inplace=True)\n",
    "stage_date = ['03/01/22', '02/22/22', '03/08/22']\n",
    "flows = flows.loc[flows['date'].isin(stage_date)]\n",
    "# label the regional level\n",
    "flows = gpd.sjoin(flows, inoutter, how='inner', predicate='within')\n",
    "flows = flows.drop(columns=['index_right', 'Source', 'Area_Ha', 'Shape_Leng', 'Shape_Area'])\n",
    "flows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "merged = flows\n",
    "\n",
    "# convert the dataframe\n",
    "flows = pd.melt(flows,\n",
    "                id_vars=['toid', 'classification', 'geometry', 'date', 'Boundary', 'cycle_lane', 'bus_lane', 'boroughs'],\n",
    "                var_name='mode', value_name='flow')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:38:54.201959Z",
     "start_time": "2023-08-13T20:34:27.252444Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "date                     toid        mode classification  \\\n0        osgb4000000027865921         bus       Motorway   \n1        osgb4000000027865921         car       Motorway   \n2        osgb4000000027865921       cycle       Motorway   \n3        osgb4000000027865921  stationary       Motorway   \n4        osgb4000000027865921       total       Motorway   \n...                       ...         ...            ...   \n1742635  osgb5000005242182149         car     Local Road   \n1742636  osgb5000005242182149       cycle     Local Road   \n1742637  osgb5000005242182149  stationary     Local Road   \n1742638  osgb5000005242182149       total     Local Road   \n1742639  osgb5000005242182149       walks     Local Road   \n\ndate                                              geometry      Boundary  \\\n0        LINESTRING (531539.442 200769.874, 531592.988 ...  Outer London   \n1        LINESTRING (531539.442 200769.874, 531592.988 ...  Outer London   \n2        LINESTRING (531539.442 200769.874, 531592.988 ...  Outer London   \n3        LINESTRING (531539.442 200769.874, 531592.988 ...  Outer London   \n4        LINESTRING (531539.442 200769.874, 531592.988 ...  Outer London   \n...                                                    ...           ...   \n1742635  LINESTRING (543548.945 179236.408, 543554.000 ...  Inner London   \n1742636  LINESTRING (543548.945 179236.408, 543554.000 ...  Inner London   \n1742637  LINESTRING (543548.945 179236.408, 543554.000 ...  Inner London   \n1742638  LINESTRING (543548.945 179236.408, 543554.000 ...  Inner London   \n1742639  LINESTRING (543548.945 179236.408, 543554.000 ...  Inner London   \n\ndate    cycle_lane bus_lane   boroughs  03/01/22  02/22/22  03/08/22  \\\n0                n        n    Enfield        16        11        12   \n1                n        n    Enfield      1041      1100      1081   \n2                n        n    Enfield        14         4         7   \n3                n        n    Enfield         2         0         1   \n4                n        n    Enfield      1095      1151      1122   \n...            ...      ...        ...       ...       ...       ...   \n1742635          n        n  Greenwich        33        54        51   \n1742636          n        n  Greenwich         2         1         1   \n1742637          n        n  Greenwich         3         4         5   \n1742638          n        n  Greenwich        50        74        70   \n1742639          n        n  Greenwich        11        14        12   \n\ndate     impact_flow  recovery_flow  impact_rate  recovery_rate  \n0                  5             -4       0.4545        -0.2500  \n1                -59             40      -0.0536         0.0384  \n2                 10             -7       2.5000        -0.5000  \n3                  2             -1       0.0000        -0.5000  \n4                -56             27      -0.0487         0.0247  \n...              ...            ...          ...            ...  \n1742635          -21             18      -0.3889         0.5455  \n1742636            1             -1       1.0000        -0.5000  \n1742637           -1              2      -0.2500         0.6667  \n1742638          -24             20      -0.3243         0.4000  \n1742639           -3              1      -0.2143         0.0909  \n\n[1742640 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>date</th>\n      <th>toid</th>\n      <th>mode</th>\n      <th>classification</th>\n      <th>geometry</th>\n      <th>Boundary</th>\n      <th>cycle_lane</th>\n      <th>bus_lane</th>\n      <th>boroughs</th>\n      <th>03/01/22</th>\n      <th>02/22/22</th>\n      <th>03/08/22</th>\n      <th>impact_flow</th>\n      <th>recovery_flow</th>\n      <th>impact_rate</th>\n      <th>recovery_rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>osgb4000000027865921</td>\n      <td>bus</td>\n      <td>Motorway</td>\n      <td>LINESTRING (531539.442 200769.874, 531592.988 ...</td>\n      <td>Outer London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Enfield</td>\n      <td>16</td>\n      <td>11</td>\n      <td>12</td>\n      <td>5</td>\n      <td>-4</td>\n      <td>0.4545</td>\n      <td>-0.2500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>osgb4000000027865921</td>\n      <td>car</td>\n      <td>Motorway</td>\n      <td>LINESTRING (531539.442 200769.874, 531592.988 ...</td>\n      <td>Outer London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Enfield</td>\n      <td>1041</td>\n      <td>1100</td>\n      <td>1081</td>\n      <td>-59</td>\n      <td>40</td>\n      <td>-0.0536</td>\n      <td>0.0384</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>osgb4000000027865921</td>\n      <td>cycle</td>\n      <td>Motorway</td>\n      <td>LINESTRING (531539.442 200769.874, 531592.988 ...</td>\n      <td>Outer London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Enfield</td>\n      <td>14</td>\n      <td>4</td>\n      <td>7</td>\n      <td>10</td>\n      <td>-7</td>\n      <td>2.5000</td>\n      <td>-0.5000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>osgb4000000027865921</td>\n      <td>stationary</td>\n      <td>Motorway</td>\n      <td>LINESTRING (531539.442 200769.874, 531592.988 ...</td>\n      <td>Outer London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Enfield</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>0.0000</td>\n      <td>-0.5000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>osgb4000000027865921</td>\n      <td>total</td>\n      <td>Motorway</td>\n      <td>LINESTRING (531539.442 200769.874, 531592.988 ...</td>\n      <td>Outer London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Enfield</td>\n      <td>1095</td>\n      <td>1151</td>\n      <td>1122</td>\n      <td>-56</td>\n      <td>27</td>\n      <td>-0.0487</td>\n      <td>0.0247</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1742635</th>\n      <td>osgb5000005242182149</td>\n      <td>car</td>\n      <td>Local Road</td>\n      <td>LINESTRING (543548.945 179236.408, 543554.000 ...</td>\n      <td>Inner London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Greenwich</td>\n      <td>33</td>\n      <td>54</td>\n      <td>51</td>\n      <td>-21</td>\n      <td>18</td>\n      <td>-0.3889</td>\n      <td>0.5455</td>\n    </tr>\n    <tr>\n      <th>1742636</th>\n      <td>osgb5000005242182149</td>\n      <td>cycle</td>\n      <td>Local Road</td>\n      <td>LINESTRING (543548.945 179236.408, 543554.000 ...</td>\n      <td>Inner London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Greenwich</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>1.0000</td>\n      <td>-0.5000</td>\n    </tr>\n    <tr>\n      <th>1742637</th>\n      <td>osgb5000005242182149</td>\n      <td>stationary</td>\n      <td>Local Road</td>\n      <td>LINESTRING (543548.945 179236.408, 543554.000 ...</td>\n      <td>Inner London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Greenwich</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>-1</td>\n      <td>2</td>\n      <td>-0.2500</td>\n      <td>0.6667</td>\n    </tr>\n    <tr>\n      <th>1742638</th>\n      <td>osgb5000005242182149</td>\n      <td>total</td>\n      <td>Local Road</td>\n      <td>LINESTRING (543548.945 179236.408, 543554.000 ...</td>\n      <td>Inner London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Greenwich</td>\n      <td>50</td>\n      <td>74</td>\n      <td>70</td>\n      <td>-24</td>\n      <td>20</td>\n      <td>-0.3243</td>\n      <td>0.4000</td>\n    </tr>\n    <tr>\n      <th>1742639</th>\n      <td>osgb5000005242182149</td>\n      <td>walks</td>\n      <td>Local Road</td>\n      <td>LINESTRING (543548.945 179236.408, 543554.000 ...</td>\n      <td>Inner London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Greenwich</td>\n      <td>11</td>\n      <td>14</td>\n      <td>12</td>\n      <td>-3</td>\n      <td>1</td>\n      <td>-0.2143</td>\n      <td>0.0909</td>\n    </tr>\n  </tbody>\n</table>\n<p>1742640 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flows = pd.pivot_table(flows,\n",
    "                       index=['toid'],\n",
    "                       columns='date',\n",
    "                       values='flow',\n",
    "                       aggfunc='first').reset_index()\n",
    "\n",
    "flows['classification'] = flows['toid'].map('classification', 'geometry',  'Boundary', 'mode', 'cycle_lane', 'bus_lane', 'boroughs')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:38:54.687563Z",
     "start_time": "2023-08-13T20:38:54.201186Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "flows.drop(columns=['date'], inplace=True)\n",
    "flows = flows.groupby(\n",
    "    ['toid', 'mode', 'classification', 'geometry','Boundary', 'cycle_lane', 'bus_lane', 'boroughs'], as_index=False).agg(\n",
    "    {'03/01/22': 'first', '02/22/22': 'first', '03/08/22': 'first'})\n",
    "# Calculate the impact and recovery flows for one strike\n",
    "flows['impact_flow'] = flows['03/01/22'] - flows['02/22/22']\n",
    "flows['recovery_flow'] = flows['03/08/22'] - flows['03/01/22']\n",
    "\n",
    "# Calculate impact rate while avoiding division by zero\n",
    "flows['impact_rate'] = flows.apply(lambda row: round(row['impact_flow'] / row['02/22/22'], 4) if row['02/22/22'] != 0 else 0, axis=1)\n",
    "# Calculate recovery rate while avoiding division by zero\n",
    "flows['recovery_rate'] = flows.apply(lambda row: round(row['recovery_flow'] / row['03/01/22'], 4) if row['03/01/22'] != 0 else 0, axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T00:35:55.535006Z",
     "start_time": "2023-08-13T00:35:38.759078Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "All = flows.copy()\n",
    "All"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:38:54.977447Z",
     "start_time": "2023-08-13T20:38:54.775943Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graph"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:39:26.483550Z",
     "start_time": "2023-08-13T20:38:54.981548Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "flows = flows[flows['mode'] == 'total']\n",
    "flows.reset_index(drop=True)\n",
    "\n",
    "# Create an empty graph\n",
    "graph = nx.Graph()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:39:26.489722Z",
     "start_time": "2023-08-13T20:39:26.486912Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# creat a blank graph\n",
    "graph = nx.Graph()\n",
    "\n",
    "# iterate over the rows of the flows DataFrame\n",
    "for _, row in flows.iterrows():\n",
    "    geometry = row['geometry']\n",
    "    baseline_1 = row['02/22/22']\n",
    "    strike_1 = row['03/01/22']\n",
    "    recovery_1 = row['03/08/22']\n",
    "    impact_flow = row['impact_flow']\n",
    "    recovery_flow = row['recovery_flow']\n",
    "    direction = row['directiona']\n",
    "\n",
    "    # break the MultiLineString geometry into its constituent LineStrings\n",
    "    if geometry.geom_type == 'MultiLineString':\n",
    "        for line_string in geometry.geoms:  # iterate over each LineString\n",
    "            from_node = line_string.coords[0]\n",
    "            to_node = line_string.coords[-1]\n",
    "\n",
    "            # Add nodes to the graph\n",
    "            graph.add_node(from_node, pos=from_node)  # Use 'from_node' as the node position\n",
    "            graph.add_node(to_node, pos=to_node)  # Use 'to_node' as the node position\n",
    "            # Add edges to the graph based on the direction\n",
    "\n",
    "            graph.add_edge(to_node, from_node, baseline_1=baseline_1, strike_1=strike_1,\n",
    "                           recovery_1=recovery_1, impact_flow=impact_flow, recovery_flow=recovery_flow,\n",
    "                           toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "\n",
    "            '''if direction == 'bothDirections':\n",
    "                # If the road is bidirectional, flows are split equally in both directions\n",
    "                graph.add_edge(from_node, to_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                               recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                               toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "                graph.add_edge(to_node,\n",
    "                               from_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2, recovery_1=recovery_1 / 2,\n",
    "                               impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                               toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "            elif direction == 'inOppositeDirection':\n",
    "            # If the road is in the opposite direction, flows are from the ending point to the starting point\n",
    "\n",
    "            elif direction == 'inDirection':\n",
    "                # If the road is in the same direction, flows are from the starting point to the ending point\n",
    "                graph.add_edge(from_node, to_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                               recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                               toid=row['toid'], classification=row['classification'], geometry=row['geometry'])'''\n",
    "\n",
    "    else:\n",
    "        from_node = geometry.coords[0]\n",
    "        to_node = geometry.coords[-1]\n",
    "\n",
    "        # Add nodes to the graph\n",
    "        graph.add_node(from_node, pos=from_node)  # Use 'from_node' as the node position\n",
    "        graph.add_node(to_node, pos=to_node)  # Use 'to_node' as the node position\n",
    "        # Add edges to the graph based on the direction\n",
    "\n",
    "        graph.add_edge(to_node, from_node, baseline_1=baseline_1, strike_1=strike_1, recovery_1=recovery_1,\n",
    "                       impact_flow=impact_flow, recovery_flow=recovery_flow, toid=row['toid'],\n",
    "                       classification=row['classification'], geometry=row['geometry'])\n",
    "\n",
    "    ''' \n",
    "     if direction == 'bothDirections':\n",
    "         # If the road is bidirectional, flows are split equally in both directions\n",
    "         graph.add_edge(from_node, to_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                        recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                        toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "         graph.add_edge(to_node, from_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                        recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                        toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "     elif direction == 'inOppositeDirection':\n",
    "         # If the road is in the opposite direction, flows are from the ending point to the starting point\n",
    "         graph.add_edge(to_node, from_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                        recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                        toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "     elif direction == 'inDirection':\n",
    "         # If the road is in the same direction, flows are from the starting point to the ending point\n",
    "         graph.add_edge(from_node, to_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                        recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                        toid=row['toid'], classification=row['classification'], geometry=row['geometry'])'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:39:26.494299Z",
     "start_time": "2023-08-13T20:39:26.490328Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "\n",
    "def graph_visualization(graph, weight=None, cmap='Greens'):\n",
    "    # obtain all the edges in the graph\n",
    "    edges_list = list(graph.edges())\n",
    "\n",
    "    # make sure the graph is not empty\n",
    "    if edges_list:\n",
    "        # obtain the start and end coordinates of the first edge\n",
    "        u, v = edges_list[0]\n",
    "\n",
    "        # obtain the edge attribute of the first edge\n",
    "        edge_attr = graph[u][v]\n",
    "\n",
    "        print(f\"The first edge: ({u}, {v})\")\n",
    "        print(\"Attribute: \", edge_attr)\n",
    "    else:\n",
    "        print(\"No edge in the graph.\")\n",
    "\n",
    "    # get the node positions\n",
    "    node_positions = nx.get_node_attributes(graph, 'pos')\n",
    "\n",
    "    # get the edge weights\n",
    "    edge_weight = nx.get_edge_attributes(graph, weight)\n",
    "\n",
    "    # normalize the edge weights between 0 and 1\n",
    "    weight_values = list(edge_weight.values())\n",
    "    norm = Normalize(vmin=min(weight_values), vmax=max(weight_values))\n",
    "    norm_weight = {edge: norm(weight) for edge, weight in edge_weight.items()}\n",
    "\n",
    "    # create a colormap\n",
    "    cmap_object = plt.get_cmap(cmap)\n",
    "    mappable = ScalarMappable(norm=norm, cmap=cmap_object)\n",
    "    mappable.set_array([])\n",
    "\n",
    "    # plot the graph\n",
    "    fig, ax = plt.subplots()\n",
    "    nx.draw_networkx_edges(graph, pos=node_positions, edge_color='gray')\n",
    "    nx.draw_networkx_edges(graph, pos=node_positions,\n",
    "                           edge_color=[mappable.to_rgba(norm_weight[edge]) for edge in edges_list])\n",
    "\n",
    "    # add a colorbar\n",
    "    cbar = plt.colorbar(mappable, ax=ax, orientation='horizontal', pad=0.01)\n",
    "    cbar.set_label(weight)\n",
    "\n",
    "    plt.title(f'Graph Representation of the Road Network', size=10)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 48\u001B[0m\n\u001B[1;32m     45\u001B[0m average_degree \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;28mdict\u001B[39m(node_degrees)\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(node_degrees)\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# calculate the betweenness_centrality for each road\u001B[39;00m\n\u001B[0;32m---> 48\u001B[0m betweenness_centrality \u001B[38;5;241m=\u001B[39m \u001B[43mnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbetweenness_centrality\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# Add the calculated indicators as attributes for each road\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m u, v \u001B[38;5;129;01min\u001B[39;00m graph\u001B[38;5;241m.\u001B[39medges():\n",
      "File \u001B[0;32m/opt/anaconda3/envs/geospatial/lib/python3.8/site-packages/networkx/utils/decorators.py:845\u001B[0m, in \u001B[0;36margmap.__call__.<locals>.func\u001B[0;34m(_argmap__wrapper, *args, **kwargs)\u001B[0m\n\u001B[1;32m    844\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(\u001B[38;5;241m*\u001B[39margs, __wrapper\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 845\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43margmap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_lazy_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43m__wrapper\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<class 'networkx.utils.decorators.argmap'> compilation 12:4\u001B[0m, in \u001B[0;36margmap_betweenness_centrality_9\u001B[0;34m(G, k, normalized, weight, endpoints, seed)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgzip\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01minspect\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/geospatial/lib/python3.8/site-packages/networkx/algorithms/centrality/betweenness.py:137\u001B[0m, in \u001B[0;36mbetweenness_centrality\u001B[0;34m(G, k, normalized, weight, endpoints, seed)\u001B[0m\n\u001B[1;32m    135\u001B[0m         betweenness, _ \u001B[38;5;241m=\u001B[39m _accumulate_endpoints(betweenness, S, P, sigma, s)\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 137\u001B[0m         betweenness, _ \u001B[38;5;241m=\u001B[39m \u001B[43m_accumulate_basic\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbetweenness\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mP\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msigma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;66;03m# rescaling\u001B[39;00m\n\u001B[1;32m    139\u001B[0m betweenness \u001B[38;5;241m=\u001B[39m _rescale(\n\u001B[1;32m    140\u001B[0m     betweenness,\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28mlen\u001B[39m(G),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    145\u001B[0m     endpoints\u001B[38;5;241m=\u001B[39mendpoints,\n\u001B[1;32m    146\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/geospatial/lib/python3.8/site-packages/networkx/algorithms/centrality/betweenness.py:323\u001B[0m, in \u001B[0;36m_accumulate_basic\u001B[0;34m(betweenness, S, P, sigma, s)\u001B[0m\n\u001B[1;32m    321\u001B[0m coeff \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m delta[w]) \u001B[38;5;241m/\u001B[39m sigma[w]\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m P[w]:\n\u001B[0;32m--> 323\u001B[0m     delta[v] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m sigma[v] \u001B[38;5;241m*\u001B[39m coeff\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m w \u001B[38;5;241m!=\u001B[39m s:\n\u001B[1;32m    325\u001B[0m     betweenness[w] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m delta[w]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "graph_visualization(graph, weight='impact_flow')\n",
    "graph_visualization(graph, weight='recovery_flow')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:43:23.108906Z",
     "start_time": "2023-08-13T20:39:26.495048Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Calculate the structure-based indicators for roads and system\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate the number of nodes\n",
    "nodes_number = graph.number_of_nodes()\n",
    "\n",
    "# calculate the number of edges\n",
    "links_number = graph.number_of_edges()\n",
    "\n",
    "# calculate the total link weight\n",
    "total_link_weight = sum([data['impact_flow'] for u, v, data in graph.edges(data=True)])\n",
    "\n",
    "mean_link_weight = total_link_weight / links_number\n",
    "\n",
    "# calculate the coefficient of variation of node degree\n",
    "node_degrees = dict(graph.degree())\n",
    "mean_node_degree = np.mean(list(node_degrees.values()))\n",
    "std_node_degree = np.std(list(node_degrees.values()))\n",
    "node_degree_cv = (std_node_degree / mean_node_degree) * 100\n",
    "\n",
    "# calculate the coefficient of variation of edge weight\n",
    "edge_weights = nx.get_edge_attributes(graph, 'impact_flow').values()\n",
    "mean_edge_weight = np.mean(list(edge_weights))\n",
    "std_edge_weight = np.std(list(edge_weights))\n",
    "edge_weight_cv = (std_edge_weight / mean_edge_weight) * 100\n",
    "\n",
    "# calculate the network connectivity and score for graph\n",
    "network_connectivity = nx.is_connected(graph)\n",
    "connectivity_score = 2 * links_number / (nodes_number * nodes_number)\n",
    "\n",
    "# calculate the average clustering coefficient for graph\n",
    "avg_clustering_coefficient = nx.average_clustering(graph)\n",
    "\n",
    "# calculate the transitivity for graph\n",
    "transitivity = nx.transitivity(graph)\n",
    "\n",
    "# calculate the assortativity for graph\n",
    "assortativity = nx.degree_assortativity_coefficient(graph)\n",
    "\n",
    "# calculate indicators as attributes for each road\n",
    "clustering_coefficients = nx.clustering(graph)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(graph)\n",
    "\n",
    "# calculate the node degrees\n",
    "node_degrees = graph.degree()\n",
    "\n",
    "# calculate the average degree for graph\n",
    "average_degree = sum(dict(node_degrees).values()) / len(node_degrees)\n",
    "\n",
    "# calculate the betweenness_centrality for each road\n",
    "betweenness_centrality = nx.betweenness_centrality(graph)\n",
    "\n",
    "# Add the calculated indicators as attributes for each road\n",
    "for u, v in graph.edges():\n",
    "    edge_clustering_coefficient = (clustering_coefficients[u] + clustering_coefficients[v]) / 2\n",
    "    graph[u][v]['clustering_coefficient'] = edge_clustering_coefficient\n",
    "\n",
    "    graph[u][v]['degree'] = (node_degrees[u] + node_degrees[v]) / 2\n",
    "    graph[u][v]['betweenness'] = (betweenness_centrality[u] + betweenness_centrality[v]) / 2\n",
    "\n",
    "    edge_eigenvector_centrality = (eigenvector_centrality[u] + eigenvector_centrality[v]) / 2\n",
    "    graph[u][v]['eigenvector_centrality'] = edge_eigenvector_centrality\n",
    "\n",
    "# Print the calculated indicators\n",
    "print(\"Total Nodes Number:\", nodes_number)\n",
    "print(\"Total Links Number:\", links_number)\n",
    "print(\"Total Flows:\", round(total_link_weight))\n",
    "print(\"Mean Link Flow:\", mean_link_weight)\n",
    "print(\"Node Degree Coefficient of Variation:\", node_degree_cv)\n",
    "print(\"Edge Weight Coefficient of Variation:\", edge_weight_cv)\n",
    "print(\"Connectivity Score:\", connectivity_score)\n",
    "print(\"Network Connectivity:\", network_connectivity)\n",
    "print(\"Transitivity:\", transitivity)\n",
    "print(\"Assortativity:\", assortativity)\n",
    "print(\"Average Clustering Coefficient:\", avg_clustering_coefficient)\n",
    "print(\"Average Degree:\", average_degree)\n",
    "print(\"Average Betweenness Centrality:\", sum(betweenness_centrality.values()) / len(betweenness_centrality))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "impact_flow复原性和脆弱性分析：\n",
    "\n",
    "**复原性分析：**\n",
    "- **Total Nodes Number: 1104** 和 **Total Links Number: 1227** 表示路网的规模。规模较大可能需要更多资源来维护，但在复原性方面，如果网络中有多个节点和连接，也可能有更多的备选路径。\n",
    "\n",
    "- **Total Flows: -36378** 和 **Mean Link Flow: -29.65** 表示流量变化差值和平均链路流量。负数的流量变化差值可能表示流量减少。这可能会导致某些连接拥塞或资源分配不当，对于复原性可能带来一些挑战。\n",
    "\n",
    "- **Node Degree Coefficient of Variation: 43.65** 和 **Edge Weight Coefficient of Variation: -449.42** 表示节点度数和边权重的变异程度。负的边权重变异系数可能反映了权重值的分布不均匀。较高的节点度数变异系数可能意味着一些节点的连接更为密集，但是这种变异可能影响网络的复原性。\n",
    "\n",
    "- **Transitivity: 0.064** 表示节点的传递性。传递性的增加可能有助于信息或流量在网络中的快速传播，提高复原性。\n",
    "\n",
    "- **Assortativity: 0.032** 表示网络的度同配性。这种度同配性可能有助于信息在网络中传播，但是取值较低，作用可能不太明显。\n",
    "\n",
    "**脆弱性分析：**\n",
    "- **Connectivity Score: 0.00201** 和 **Network Connectivity: False** 表示连接性分数较低和网络不是全连通的。这可能意味着网络中可能存在一些孤立的区域，当这些区域受到影响时，整个网络的连通性会受到影响。\n",
    "\n",
    "- **Average Clustering Coefficient: 0.036** 表示平均聚类系数。较低的聚类系数可能意味着节点之间关联性不高，可能导致信息难以在网络中传播。\n",
    "\n",
    "- **Average Betweenness Centrality: 0.014** 表示平均介数中心性，较低的介数中心性可能表示网络中的信息流和影响受限。\n",
    "\n",
    "综合分析，以流量变化差值为权重的城市伦敦路网结构可能具有一定的复原性挑战和脆弱性。负数的流量变化可能表明某些区域可能存在拥塞或资源分配不足。节点度数和边权重的变异可能意味着网络中存在一些密集连接的节点和不均匀的权重分布，这可能会影响网络的鲁棒性。然而，一些正向因素如传递性、度同配性等可能有助于信息传播和一些程度的复原。在增强路网的复原性和脆弱性方面，可能需要考虑网络的连通性、资源分配、信息传播等因素。"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:43:23.108631Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Strike_1复原性分析：**\n",
    "- **Total Nodes Number: 1104** 和 **Total Links Number: 1227** 表示路网的规模。路网的大小可能对于复原性有影响，大规模路网可能需要更多的资源来维持。\n",
    "\n",
    "- **Node Degree Coefficient of Variation: 43.65** 和 **Edge Weight Coefficient of Variation: 115.01** 表示节点度数和边权重的变异程度。较高的变异系数可能意味着部分节点度数和边权重变化较大，这可能增加了复原过程的难度，因为某些节点或边更容易受到影响。\n",
    "\n",
    "- **Transitivity: 0.064** 表示节点的传递性。传递性的增加可能有助于信息或流量在网络中的快速传播，这对复原性可能有积极影响。\n",
    "\n",
    "- **Assortativity: 0.032** 表示网络的度同配性，即高度连接的节点倾向于连接到其他高度连接的节点。这种连接模式可能有助于信息在网络中传播，从而提高复原性。\n",
    "\n",
    "**脆弱性分析：**\n",
    "- **Network Connectivity: False** 表示网络不是全连通的，可能存在多个网络组件。这可能导致某些区域或网络组件受到故障影响时，整个网络的连通性受到影响。\n",
    "\n",
    "- **Connectivity Score: 0.00201** 表示网络的连接性分数较低。低连接性分数可能意味着网络中的连接较弱，复原能力较弱。\n",
    "\n",
    "- **Average Clustering Coefficient: 0.036** 表示平均聚类系数。较低的聚类系数可能表示节点之间的关联性不高，网络的鲁棒性可能较弱。\n",
    "\n",
    "- **Average Betweenness Centrality: 0.014** 表示平均介数中心性，介数中心性衡量节点在网络中作为桥梁的程度。较低的介数中心性可能意味着网络中的信息流和影响可能受限。\n",
    "\n",
    "综合分析，CoL路网结构可能具有一定的复原性挑战和脆弱性。较高的节点度数和边权重变异系数、低连接性分数、不高的平均聚类系数和介数中心性可能意味着在面对故障、拥塞、攻击等情况时，网络的复原能力可能受到一定影响。但是，度同配性、传递性等正向因素可能有助于一些程度上的复原和信息传播。为了增强路网的复原性和脆弱性，可能需要关注网络连接性、鲁棒性、信息传播能力等方面的改进。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_visualization(graph, weight='betweenness')\n",
    "graph_visualization(graph, weight='clustering_coefficient')\n",
    "graph_visualization(graph, weight='eigenvector_centrality')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:43:23.110597Z",
     "start_time": "2023-08-13T20:43:23.110442Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network DBSCAN Clustering"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:43:23.112141Z",
     "start_time": "2023-08-13T20:43:23.111890Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# \n",
    "# # 将边的权重属性值作为数据点\n",
    "# edges_list = list(graph.edges())\n",
    "# data_points = np.array([graph[u][v]['impact_flow'] for u, v in edges_list]).reshape(-1, 1)\n",
    "# # 创建模型\n",
    "# dbscan = DBSCAN()\n",
    "# \n",
    "# # 定义要搜索的参数范围\n",
    "# param_grid = {'eps': [1, 1.5, 2, 2.5, 3, 4, 5, 6, 7, 8, 9, 10], 'min_samples': [1, 2, 3, 4, 5, 6]}\n",
    "# \n",
    "# best_score = -1\n",
    "# best_eps = None\n",
    "# best_min_samples = None\n",
    "# \n",
    "# # 在数据上执行交叉验证\n",
    "# for eps in param_grid['eps']:\n",
    "#     for min_samples in param_grid['min_samples']:\n",
    "#         dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "#         labels = dbscan.fit_predict(data_points)\n",
    "#         if len(set(labels)) > 1:  # 忽略只有一个簇的情况\n",
    "#             score = silhouette_score(data_points, labels)\n",
    "#             if score > best_score:\n",
    "#                 best_score = score\n",
    "#                 best_eps = eps\n",
    "#                 best_min_samples = min_samples\n",
    "# \n",
    "# print(\"Best EPS:\", best_eps)\n",
    "# print(\"Best Min Samples:\", best_min_samples)\n",
    "# \n",
    "# # 使用最佳参数进行DBSCAN聚类\n",
    "# best_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "# best_labels = best_dbscan.fit_predict(data_points)\n",
    "# \n",
    "# # 将聚类结果应用于图\n",
    "# for i, (u, v) in enumerate(edges_list):\n",
    "#     graph[u][v]['cluster_DB'] = best_labels[i]\n",
    "# \n",
    "# # 打印聚类的唯一值\n",
    "# unique_clusters = set(best_labels)\n",
    "# print(\"Unique Cluster Values:\", unique_clusters)\n",
    "# \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 创建颜色映射\n",
    "# cmap = plt.get_cmap('tab20', len(unique_clusters))\n",
    "# \n",
    "# # 绘制图形\n",
    "# fig, ax = plt.subplots()\n",
    "pos_cluster = nx.fruchterman_reingold_layout(graph)\n",
    "# \n",
    "# for u, v, attr in graph.edges(data=True):\n",
    "#     cluster = attr['cluster_DB']\n",
    "#     if cluster == -1:\n",
    "#         edge_color = 'lightgrey'  # 将 -1 标签的边设置为浅灰色\n",
    "#     else:\n",
    "#         edge_color = cmap(cluster)\n",
    "#     nx.draw_networkx_edges(graph, pos_cluster, edgelist=[(u, v)], width=1, edge_color=edge_color)\n",
    "# \n",
    "# # 创建不连续的分类点图例\n",
    "# unique_labels = np.unique(best_labels)\n",
    "# handles = []\n",
    "# for label in unique_labels:\n",
    "#     if label == -1:  # 处理 -1 标签\n",
    "#         handle = plt.Line2D([], [], color='lightgrey', marker='o', markersize=10, label='Noise')\n",
    "#     else:\n",
    "#         color = cmap(label)\n",
    "#         handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "#     handles.append(handle)\n",
    "# \n",
    "# # 添加图例\n",
    "# ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "# \n",
    "# plt.axis('off')\n",
    "# plt.title(\"DBSCAN Clustering of Graph Edges\")\n",
    "# plt.show()\n",
    "# \n",
    "# node_positions = nx.get_node_attributes(graph, 'pos')\n",
    "# nx.draw_networkx_edges(graph, node_positions, width=1, edge_color=cmap(best_labels))\n",
    "# plt.title(\"DBSCAN Clustering in network\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network K-Means Clustering\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:43:23.113660Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### By Degree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# 从图中获取节点的度作为特征\n",
    "node_features = np.array([graph.degree(node) for node in graph.nodes()]).reshape(-1, 1)\n",
    "# 使用 Z-Score 标准化对节点特征进行归一化\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(node_features)\n",
    "\n",
    "# 使用 Min-Max 缩放对节点特征进行归一化到 [0, 1] 范围\n",
    "minmax_scaler = MinMaxScaler()\n",
    "node_features = minmax_scaler.fit_transform(node_features)\n",
    "\n",
    "# 使用K均值算法进行聚类\n",
    "k = 7  # 聚类数量\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "cluster_labels = kmeans.fit_predict(node_features)\n",
    "\n",
    "# 将聚类结果应用于图\n",
    "for i, node in enumerate(graph.nodes()):\n",
    "    graph.nodes[node]['cluster_k'] = cluster_labels[i]\n",
    "\n",
    "# 初始化节点的累计flow和度的字典\n",
    "node_weight = {node: 0 for node in graph.nodes()}\n",
    "node_degrees = {node: 0 for node in graph.nodes()}\n",
    "\n",
    "# 遍历边，累计flow和度\n",
    "for u, v, attr in graph.edges(data=True):\n",
    "    weight = attr['impact_rate']\n",
    "    node_weight[u] += weight\n",
    "    node_weight[v] += weight\n",
    "    node_degrees[u] += 1\n",
    "    node_degrees[v] += 1\n",
    "\n",
    "# 计算每个节点的特征（平均flow）\n",
    "node_features = np.array(\n",
    "    [node_weight[node] / node_degrees[node] if node_degrees[node] > 0 else 0 for node in graph.nodes()]).reshape(-1, 1)\n",
    "\n",
    "# 创建颜色映射\n",
    "cmap = plt.get_cmap('tab20', k)\n",
    "\n",
    "# 绘制图形\n",
    "fig, ax = plt.subplots()\n",
    "pos = nx.fruchterman_reingold_layout(graph)\n",
    "\n",
    "for u, v, attr in graph.edges(data=True):\n",
    "    u_cluster = graph.nodes[u]['cluster_k']\n",
    "    v_cluster = graph.nodes[v]['cluster_k']\n",
    "\n",
    "    if u_cluster == v_cluster:\n",
    "        cluster = u_cluster\n",
    "    else:\n",
    "        cluster = -1  # 表示不同的聚类\n",
    "    if cluster == -1:\n",
    "        edge_color = 'lightgrey'  # 将不同聚类的边设置为浅灰色\n",
    "    else:\n",
    "        edge_color = cmap(cluster)\n",
    "    nx.draw_networkx_edges(graph, pos_cluster, edgelist=[(u, v)], width=1, edge_color=edge_color)\n",
    "\n",
    "# 创建不连续的分类点图例\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "handles = []\n",
    "for label in unique_labels:\n",
    "    if label == -1:  # 处理 -1 标签\n",
    "        handle = plt.Line2D([], [], color='lightgrey', marker='o', markersize=10, label='Noise')\n",
    "    else:\n",
    "        color = cmap(label)\n",
    "        handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "    handles.append(handle)\n",
    "\n",
    "# 添加图例\n",
    "ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1)).set_draggable(True)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title(\"KMeans Clustering of Graph Nodes\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the clusters in the network\n",
    "node_positions = nx.get_node_attributes(graph, 'pos')\n",
    "nx.draw_networkx_edges(graph, node_positions, width=1, edge_color=cmap(cluster_labels))\n",
    "# 创建不连续的分类点图例\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "handles = []\n",
    "for label in unique_labels:\n",
    "    if label == -1:  # 处理 -1 标签\n",
    "        handle = plt.Line2D([], [], color='lightgrey', marker='o', markersize=10, label='Noise')\n",
    "    else:\n",
    "        color = cmap(label)\n",
    "        handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "    handles.append(handle)\n",
    "\n",
    "# 添加图例\n",
    "ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1)).set_draggable(True)\n",
    "plt.title(\"KMeans Clustering in network\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:43:23.115105Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### By Flow changes / Clustering Coefficient"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:43:23.116011Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 获取所有边的列表\n",
    "edges_list = list(graph.edges())\n",
    "\n",
    "# 按照flows聚类\n",
    "# 初始化特征矩阵\n",
    "num_edges = len(edges_list)\n",
    "feature_matrix = np.zeros((num_edges, 1))  # 1列，代表 \"flow\" 属性值\n",
    "\n",
    "# 填充特征矩阵\n",
    "for i, (u, v) in enumerate(edges_list):\n",
    "    flow = graph[u][v]['impact_rate']\n",
    "    feature_matrix[i, 0] = flow\n",
    "\n",
    "# 使用 Z-Score 标准化对节点特征进行归一化\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(feature_matrix)\n",
    "\n",
    "# 使用 Min-Max 缩放对节点特征进行归一化到 [0, 1] 范围\n",
    "minmax_scaler = MinMaxScaler()\n",
    "feature_matrix = minmax_scaler.fit_transform(feature_matrix)\n",
    "\n",
    "\n",
    "# #按照Clustering Coefficient聚类\n",
    "# # 初始化特征矩阵\n",
    "# num_edges = len(edges_list)\n",
    "# num_nodes = len(graph.nodes())\n",
    "# feature_matrix = np.zeros((num_edges, num_nodes))\n",
    "# \n",
    "# # 填充特征矩阵\n",
    "# for i, (u, v) in enumerate(edges_list):\n",
    "#     u_idx = list(graph.nodes()).index(u)  # 获取节点 u 的整数索引\n",
    "#     v_idx = list(graph.nodes()).index(v)  # 获取节点 v 的整数索引\n",
    "#     u_clustering = nx.clustering(graph)[u]\n",
    "#     v_clustering = nx.clustering(graph)[v]\n",
    "#     feature_matrix[i, u_idx] = u_clustering\n",
    "#     feature_matrix[i, v_idx] = v_clustering"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:43:23.116912Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 计算不同聚类数下的误差平方和\n",
    "inertia = []\n",
    "for k in range(1, 20):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(feature_matrix)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 20), inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Optimal Cluster Number for KMeans')\n",
    "plt.xticks(range(1, 20))  # 设置横轴刻度为整数\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# 使用K均值算法进行聚类\n",
    "k = 11  # 聚类数量\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "# 将聚类结果应用于图\n",
    "for i, (u, v) in enumerate(edges_list):\n",
    "    graph[u][v]['cluster_k'] = cluster_labels[i]\n",
    "\n",
    "# 创建颜色映射\n",
    "cmap = plt.get_cmap('tab20', k)\n",
    "\n",
    "# 绘制图形\n",
    "fig, ax = plt.subplots()\n",
    "pos = nx.spring_layout(graph)\n",
    "\n",
    "for u, v, attr in graph.edges(data=True):\n",
    "    cluster = attr['cluster_k']\n",
    "    edge_color = cmap(cluster)\n",
    "    nx.draw_networkx_edges(graph, pos, edgelist=[(u, v)], width=1, edge_color=edge_color)\n",
    "\n",
    "# 创建不连续的分类点图例\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "handles = []\n",
    "for label in unique_labels:\n",
    "    color = cmap(label)\n",
    "    handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "    handles.append(handle)\n",
    "\n",
    "ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.axis('off')\n",
    "plt.title(\"Edge Flow Changes-based Clustering\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the clusters in the network\n",
    "node_positions = nx.get_node_attributes(graph, 'pos')\n",
    "nx.draw_networkx_edges(graph, node_positions, width=1, edge_color=cmap(cluster_labels))\n",
    "# 创建不连续的分类点图例\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "handles = []\n",
    "for label in unique_labels:\n",
    "    if label == -1:  # 处理 -1 标签\n",
    "        handle = plt.Line2D([], [], color='lightgrey', marker='o', markersize=10, label='Noise')\n",
    "    else:\n",
    "        color = cmap(label)\n",
    "        handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "    handles.append(handle)\n",
    "\n",
    "# 添加图例\n",
    "ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.title(\"KMeans Clustering in network\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:22:49.293771Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export the graph and update the indicators to All dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:22:49.295265Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# update = All.copy()\n",
    "# \n",
    "# # 创建一个空的 Pandas DataFrame\n",
    "# columns = ['toid', 'clustering_coefficient', 'degree', 'betweenness',\n",
    "#            'eigenvector_centrality', 'cluster_DB', 'cluster_k']\n",
    "# data = []\n",
    "# \n",
    "# # 将图的每条边导出到 DataFrame\n",
    "# for _, _, edge_data in graph.edges(data=True):\n",
    "#     row_data = {'toid': edge_data['toid'],\n",
    "#                 # 'strike_1': edge_data['strike_1'], 'baseline_1': edge_data['baseline_1'],\n",
    "#                 # 'recovery_1': edge_data['recovery_1'],\n",
    "#                 # 'impact_flow': edge_data['impact_flow'], 'recovery_flow': edge_data['recovery_flow'],\n",
    "#                 # 'classification': edge_data['classification'], 'geometry': edge_data['geometry'],\n",
    "#                 'clustering_coefficient': edge_data['clustering_coefficient'], 'degree': edge_data['degree'],\n",
    "#                 'betweenness': edge_data['betweenness'], 'eigenvector_centrality': edge_data['eigenvector_centrality'],\n",
    "#                 'cluster_DB': edge_data['cluster_DB'], 'cluster_k': edge_data['cluster_k']}\n",
    "#     data.append(row_data)\n",
    "# \n",
    "# # 创建 DataFrame\n",
    "# graph_df = pd.DataFrame(data, columns=columns)\n",
    "# graph_df['mode'] = 'total'\n",
    "# \n",
    "# update['toid'] = update['toid'].astype(str)\n",
    "# \n",
    "# update = pd.merge(update, graph_df, on=['toid', 'mode'], how='left')\n",
    "# \n",
    "# \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # need to merge all updates to All dataframe\n",
    "# update"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flow changes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:43:23.117820Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Road space reallocation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:22:49.298394Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.cm as cm\n",
    "import plotly.io as pio"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:22:49.300051Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flow_change = flows.copy()\n",
    "flow_change.drop(\n",
    "    columns={'toid', 'geometry', 'impact_flow', 'recovery_flow','impact_rate','recovery_rate'},\n",
    "    inplace=True)\n",
    "\n",
    "flow_change = flow_change.groupby(['mode', 'classification', 'Boundary']).agg(\n",
    "    {'03/01/22': 'sum', '02/22/22': 'sum', '03/08/22': 'sum'}).reset_index().rename_axis(None, axis=1)\n",
    "flow_change = flow_change.astype({'03/01/22': int, '02/22/22': int, '03/08/22': int})\n",
    "flow_change = flow_change[flow_change['mode'] != 'total']\n",
    "flow_change.insert(0, 'Total Flows', 'Total Flows')\n",
    "flow_change['impact_flow'] = flow_change['03/01/22'] - flow_change['02/22/22']\n",
    "flow_change['recovery_flow'] = flow_change['03/08/22'] - flow_change['03/01/22']\n",
    "\n",
    "# Calculate impact rate while avoiding division by zero\n",
    "flows['impact_rate'] = flows.apply(lambda row: round(row['impact_flow'] / row['02/22/22'], 4) if row['02/22/22'] != 0 else 0, axis=1)\n",
    "# Calculate recovery rate while avoiding division by zero\n",
    "flows['recovery_rate'] = flows.apply(lambda row: round(row['recovery_flow'] / row['03/01/22'], 4) if row['03/01/22'] != 0 else 0, axis=1)\n",
    "\n",
    "# 获取所有列的列表\n",
    "columns = flow_change.columns\n",
    "\n",
    "# 遍历每列，将内容转换为首字母大写\n",
    "for column in columns:\n",
    "    if flow_change[column].dtype == 'object':  # 仅对字符串列进行操作\n",
    "        flow_change[column] = flow_change[column].str.title()  # 使用str.title()函数将首字母大写\n",
    "\n",
    "# 获取除了非数值列（例如日期和字符串）之外的所有列\n",
    "numeric_columns = flow_change.select_dtypes(include=['number']).columns\n",
    "\n",
    "# 创建MinMaxScaler对象\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 使用fit_transform方法对数值列进行缩放\n",
    "flow_change[numeric_columns] = scaler.fit_transform(flow_change[numeric_columns])\n",
    "\n",
    "flow_change"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:22:49.302070Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total = ['Total Flows']\n",
    "modes = ['Bus', 'Car', 'Cycle', 'Walks', 'Stationary']\n",
    "boundary_nodes = ['Motorway', 'Strategic Road', 'Local Road', 'Inner London', 'Outer London']\n",
    "\n",
    "nodes = total + modes + boundary_nodes\n",
    "\n",
    "node_indices = {node: index for index, node in enumerate(nodes)}\n",
    "\n",
    "dates = ['03/01/22', '02/22/22', '03/08/22']\n",
    "\n",
    "# # 设置内置主题\n",
    "pio.templates.default = 'plotly'  # 可以更改为其他内置主题名称 ['plotly'：默认主题。'simple_white'：简洁的白色背景主题。'ggplot2'：仿照ggplot2的主题。'seaborn'：仿照seaborn的主题。'plotly_dark'：深色背景主题。]\n",
    "\n",
    "\n",
    "# 设置节点颜色，现有流量颜色与目标节点颜色相同\n",
    "# node_colors = ['blue', 'green', 'orange', 'red', 'purple', 'cyan', 'gray', 'pink', 'brown', 'yellow', 'magenta', 'teal']\n",
    "\n",
    "# 使用 matplotlib 的颜色映射生成节点颜色\n",
    "\n",
    "def convert_to_rgba(color, alpha):\n",
    "    return f'rgba({int(color[0] * 255)}, {int(color[1] * 255)}, {int(color[2] * 255)}, {alpha})'\n",
    "\n",
    "\n",
    "cmap = cm.get_cmap(\n",
    "    # 'twilight'\n",
    "    'tab20c'\n",
    "    , len(nodes))  # 选择一个颜色映射\n",
    "node_colors = [convert_to_rgba(cmap(i), 0.6) for i in range(len(nodes))]  # 生成对应数量的颜色\n",
    "\n",
    "for date in dates:\n",
    "    sankey_data = []\n",
    "    for i, row in flow_change.iterrows():\n",
    "        source_node = row['Total Flows']\n",
    "        target_node = row['mode']\n",
    "        value = row[date]\n",
    "\n",
    "        sankey_data.append({\n",
    "            'source': node_indices[source_node],\n",
    "            'target': node_indices[target_node],\n",
    "            'value': value,\n",
    "            'color': node_colors[node_indices[source_node]]\n",
    "        })\n",
    "\n",
    "        source_node = row['mode']\n",
    "        target_node = row['classification']\n",
    "        value = row[date]\n",
    "\n",
    "        sankey_data.append({\n",
    "            'source': node_indices[source_node],\n",
    "            'target': node_indices[target_node],\n",
    "            'value': value,\n",
    "            'color': node_colors[node_indices[source_node]]\n",
    "        })\n",
    "\n",
    "        source_node = row['classification']\n",
    "        target_node = row['Boundary']\n",
    "        value = row[date]\n",
    "\n",
    "        sankey_data.append({\n",
    "            'source': node_indices[source_node],\n",
    "            'target': node_indices[target_node],\n",
    "            'value': value,\n",
    "            'color': node_colors[node_indices[source_node]]\n",
    "        })\n",
    "\n",
    "    fig = go.Figure(go.Sankey(\n",
    "\n",
    "        arrangement='freeform',\n",
    "\n",
    "        node=dict(\n",
    "            pad=10,\n",
    "            thickness=20,\n",
    "            line=dict(color='black', width=0.3),\n",
    "            label=nodes,\n",
    "            color=node_colors\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=[link['source'] for link in sankey_data],\n",
    "            target=[link['target'] for link in sankey_data],\n",
    "            value=[link['value'] for link in sankey_data],\n",
    "            color=[link['color'] for link in sankey_data],\n",
    "        ),\n",
    "\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Road Space Reallocation on {date}\",\n",
    "        font_size=15,\n",
    "        autosize=True,\n",
    "        hovermode='closest'\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:22:49.302936Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_mode = flows.copy()\n",
    "\n",
    "class_mode_t = class_mode\n",
    "class_mode_f = class_mode\n",
    "\n",
    "class_mode_t = class_mode.groupby(['mode', 'classification', 'Boundary']).agg(\n",
    "    {'03/01/22': 'sum', '02/22/22': 'sum', '03/08/22': 'sum'}).reset_index().rename_axis(None, axis=1)\n",
    "class_mode_t = class_mode_t.astype({'03/01/22': int, '02/22/22': int, '03/08/22': int})\n",
    "\n",
    "class_mode_t['impact_flow'] = class_mode_t['03/01/22'] - class_mode_t['02/22/22']\n",
    "class_mode_t['recovery_flow'] = class_mode_t['03/08/22'] - class_mode_t['03/01/22']\n",
    "\n",
    "# 获取所有列的列表\n",
    "columns = class_mode_t.columns\n",
    "\n",
    "# 遍历每列，将内容转换为首字母大写\n",
    "for column in columns:\n",
    "    if class_mode_t[column].dtype == 'object':  # 仅对字符串列进行操作\n",
    "        class_mode_t[column] = class_mode_t[column].str.title()  # 使用str.title()函数将首字母大写\n",
    "\n",
    "class_mode_t = class_mode_t.pivot_table(index='classification', columns='mode', values=['impact_flow', 'recovery_flow'])\n",
    "# 将列名重新整理成多重索引的形式\n",
    "class_mode_t.columns = [f'{col[1]}-{col[0]}' for col in class_mode_t.columns]\n",
    "\n",
    "class_mode_t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:22:49.303979Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 获取所有列的列表\n",
    "columns = class_mode_f.columns\n",
    "\n",
    "# 遍历每列，将内容转换为首字母大写\n",
    "for column in columns:\n",
    "    if class_mode_f[column].dtype == 'object':  # 仅对字符串列进行操作\n",
    "        class_mode_f[column] = class_mode_f[column].str.title()  # 使用str.title()函数将首字母大写\n",
    "\n",
    "class_mode_f = class_mode_f.pivot_table(index=['classification', 'toid'], values=['impact_flow', 'recovery_flow'],\n",
    "                                        columns='mode')\n",
    "# 将列名重新整理成多重索引的形式\n",
    "# class_mode_f.columns = [f'{col[1]}/{col[0]}' for col in class_mode_f.columns]\n",
    "class_mode_f = class_mode_f.swaplevel(axis=1)\n",
    "\n",
    "# 定义第一层和第二层索引的顺序\n",
    "first_level_order = ['Total', 'Car', 'Bus', 'Cycle', 'Walks', 'Stationary']\n",
    "second_level_order = ['impact_flow', 'recovery_flow']\n",
    "\n",
    "# 初始化一个空列表用于存储排序后的列名\n",
    "sorted_columns = []\n",
    "\n",
    "# 循环遍历第一层索引的顺序\n",
    "for first_level in first_level_order:\n",
    "    # 循环遍历第二层索引的顺序\n",
    "    for second_level in second_level_order:\n",
    "        # 构建当前列名\n",
    "        current_column = (first_level, second_level)\n",
    "        # 将当前列名添加到排序后的列名列表中\n",
    "        sorted_columns.append(current_column)\n",
    "\n",
    "# 使用排序后的列名对 DataFrame 进行排序\n",
    "class_mode_f = class_mode_f[sorted_columns]\n",
    "\n",
    "class_mode_f\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:22:49.305309Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# 选择需要进行归一化的数值列\n",
    "numeric_columns = class_mode_f.select_dtypes(include=['number']).columns\n",
    "\n",
    "# 创建MinMaxScaler对象\n",
    "\n",
    "scaler_positive = MinMaxScaler(feature_range=(0, 1))  # 归一化到区间[0, 1]\n",
    "scaler_negative = MinMaxScaler(feature_range=(-1, 0))  # 归一化到区间[-1, 0]\n",
    "\n",
    "# 对大于等于0的数据进行归一化\n",
    "class_mode_f_positive = class_mode_f.copy()\n",
    "positive_values = class_mode_f[numeric_columns].values\n",
    "positive_mask = positive_values >= 0\n",
    "class_mode_f[numeric_columns] = np.where(positive_mask, scaler_positive.fit_transform(positive_values), positive_values)\n",
    "\n",
    "# 对小于0的数据进行归一化\n",
    "class_mode_f_negative = class_mode_f.copy()\n",
    "negative_values = class_mode_f[numeric_columns].values\n",
    "negative_mask = positive_values < 0\n",
    "class_mode_f[numeric_columns] = np.where(negative_mask, scaler_negative.fit_transform(negative_values), negative_values)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:22:49.306617Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the figure and axis\n",
    "figure, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "# Create a heatmap using Seaborn\n",
    "heatmap = plt.imshow(class_mode_f, cmap='viridis', aspect='auto', origin='upper')\n",
    "# heatmap = sns.heatmap(class_mode_f, cmap='viridis', ax=ax) \n",
    "\n",
    "# Set title\n",
    "ax.set_title(\"Impact and Recovery Flow by Mode and Classification\")\n",
    "\n",
    "# Set x-axis tick labels and rotation\n",
    "ax.set_xticks(range(len(class_mode_f.columns)))\n",
    "ax.set_xticklabels(class_mode_f.columns.get_level_values(1), rotation=30)\n",
    "\n",
    "# Set y-axis tick labels and classifications\n",
    "classifications = class_mode_f.index.get_level_values(0).unique()\n",
    "ax.set_yticks(range(len(classifications)))\n",
    "ax.set_yticklabels(classifications)\n",
    "\n",
    "# 获取分类数据的唯一值和数量\n",
    "unique_classifications, counts = np.unique(class_mode_f.index.get_level_values(0), return_counts=True)\n",
    "grouped_ticks = np.cumsum(counts) - counts / 2\n",
    "\n",
    "lines_ticks = np.cumsum(counts)\n",
    "\n",
    "# 设置左纵轴刻度\n",
    "ax.set_yticks(grouped_ticks)\n",
    "ax.set_yticklabels(unique_classifications)\n",
    "\n",
    "# 添加纵轴分割线\n",
    "for tick in lines_ticks[:-1]:\n",
    "    ax.axhline(tick, color='white', linewidth=1.2)  # 添加白色分割线，-0.5是为了使线在刻度之间\n",
    "\n",
    "# 添加颜色条\n",
    "cbar = plt.colorbar(heatmap, ax=ax)\n",
    "cbar.set_label('Flows')  # 设置颜色条标签\n",
    "\n",
    "# 移除默认分割线\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(False)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(class_mode_f.index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
