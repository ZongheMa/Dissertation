{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:34:08.423820Z",
     "start_time": "2023-08-30T14:34:08.154593Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/geospatial/lib/python3.8/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/var/folders/1k/27mkp8bj3ps60c3nmr7rbqzh0000gn/T/ipykernel_23103/1895528008.py:1: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import IPython.display as display\n",
    "import copy\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def merge_csv_files(directory):\n",
    "    # Get a list of all the csv files\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "    # Initialize an empty list to hold dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through csv files, read each into a dataframe, and append to the list\n",
    "    for file in csv_files:\n",
    "        # Extract date from filename, assuming the date is in format 'traffic_flow_YYYY_MM_DD'\n",
    "        date_str = file.split('.')[0].split('_')[-3:]  # This gives ['YYYY', 'MM', 'DD']\n",
    "        date = datetime.strptime('_'.join(date_str), '%Y_%m_%d').date()\n",
    "\n",
    "        df = pd.read_csv(os.path.join(directory, file))\n",
    "\n",
    "        # Add date as a new column\n",
    "        df['date'] = date.strftime('%m/%d/%y')\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all dataframes in the list into one dataframe\n",
    "    merged_df = pd.concat(dfs, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # Return the merged dataframe\n",
    "    return merged_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:34:10.247922Z",
     "start_time": "2023-08-30T14:34:10.236415Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "traffic_flows = merge_csv_files(\n",
    "    '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/[XH]Traffic flow')\n",
    "road_network = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/[XH]road_network/road_network.shp'\n",
    "\n",
    "# clean the traffic flow data\n",
    "traffic_flows = traffic_flows.drop_duplicates(['toid', 'date'])\n",
    "traffic_flows = traffic_flows.groupby(['toid', 'date']).agg(\n",
    "    {'bus': 'sum', 'car': 'sum', 'cycle': 'sum', 'walks': 'sum', 'stationary': 'sum'}).reset_index()\n",
    "traffic_flows['total'] = traffic_flows['bus'] + traffic_flows['car'] + traffic_flows['cycle'] + traffic_flows['walks'] + \\\n",
    "                         traffic_flows['stationary']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:34:23.016855Z",
     "start_time": "2023-08-30T14:34:10.979770Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "lsoa = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/London administrative boundaries/london_LSOA/london_LSOA.shp'\n",
    "\n",
    "inoutter = '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/London administrative boundaries/lp-consultation-oct-2009-inner-outer-london-shp/lp-consultation-oct-2009-inner-outer-london.shp'\n",
    "# tube_line = 'https://raw.githubusercontent.com/oobrien/vis/master/tubecreature/data/tfl_lines.json'\n",
    "# tube_station = 'https://raw.githubusercontent.com/oobrien/vis/master/tubecreature/data/tfl_stations.json'\n",
    "\n",
    "inoutter = gpd.read_file(inoutter)\n",
    "inoutter.to_crs(epsg=27700, inplace=True)\n",
    "\n",
    "# tube_station = gpd.read_file(tube_station)\n",
    "# tube_station.to_crs(epsg=27700, inplace=True)\n",
    "# tube_station = gpd.sjoin(tube_station, inoutter, op='within')\n",
    "\n",
    "# tube_line = gpd.read_file(tube_line)\n",
    "# tube_line.to_crs(epsg=27700, inplace=True)\n",
    "# tube_line = gpd.sjoin(tube_line, inoutter, op='within')\n",
    "\n",
    "lsoa = gpd.read_file(lsoa, crs={'init': 'epsg:27700'})\n",
    "road_network = gpd.read_file(road_network, crs={'init': 'epsg:27700'})\n",
    "road_network.rename(columns={'NAME': 'boroughs'}, inplace=True)\n",
    "road_network.loc[:, ['cycle_lane', 'bus_lane']] = road_network[['cycle_lane', 'bus_lane']].fillna('n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:35:01.216517Z",
     "start_time": "2023-08-30T14:34:23.016411Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "flows = pd.merge(\n",
    "    road_network[\n",
    "        ['toid', 'roadclassi', 'geometry', 'cycle_lane', 'bus_lane', 'boroughs']],\n",
    "    traffic_flows, left_on='toid', right_on='toid', how='left')\n",
    "flows.set_geometry('geometry', inplace=True)\n",
    "\n",
    "flows['classification'] = flows['roadclassi'].replace(\n",
    "    {'Unknown': 'Local Road', 'Not Classified': 'Local Road', 'Unclassified': 'Local Road',\n",
    "     'Classified Unnumbered': 'Local Road', 'A Road': 'Strategic Road', 'B Road': 'Strategic Road'})\n",
    "\n",
    "flows.drop(columns=['roadclassi'], inplace=True)\n",
    "\n",
    "stage_date = ['03/01/22', '02/22/22', '03/08/22']\n",
    "flows = flows.loc[flows['date'].isin(stage_date)]\n",
    "\n",
    "# label the regional level\n",
    "flows = gpd.sjoin(flows, inoutter, how='inner', predicate='within')\n",
    "flows = flows.drop(columns=['index_right', 'Source', 'Area_Ha', 'Shape_Leng', 'Shape_Area'])\n",
    "flows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "merged = flows\n",
    "flows = merged\n",
    "\n",
    "# convert the dataframe\n",
    "flows = pd.melt(flows,\n",
    "                id_vars=['toid', 'classification', 'geometry', 'date', 'Boundary', 'cycle_lane', 'bus_lane',\n",
    "                         'boroughs'],\n",
    "                var_name='mode', value_name='flow')\n",
    "\n",
    "flows = pd.pivot_table(flows,\n",
    "                       index=['toid', 'classification', 'geometry', 'Boundary', 'mode', 'cycle_lane', 'bus_lane',\n",
    "                              'boroughs'],\n",
    "                       columns='date',\n",
    "                       values='flow',\n",
    "                       aggfunc='first').reset_index()\n",
    "\n",
    "flows = flows.groupby(\n",
    "    ['toid', 'mode', 'classification', 'geometry', 'Boundary', 'cycle_lane', 'bus_lane', 'boroughs'],\n",
    "    as_index=False).agg(\n",
    "    {'03/01/22': 'first', '02/22/22': 'first', '03/08/22': 'first'})\n",
    "# Calculate the impact and recovery flows for one strike\n",
    "flows['impact_flow'] = flows['03/01/22'] - flows['02/22/22']\n",
    "flows['recovery_flow'] = flows['03/08/22'] - flows['03/01/22']\n",
    "\n",
    "# Calculate impact rate while avoiding division by zero\n",
    "flows['impact_rate'] = flows.apply(\n",
    "    lambda row: round(row['impact_flow'] / row['02/22/22'], 4) if row['02/22/22'] != 0 else 0, axis=1)\n",
    "# Calculate recovery rate while avoiding division by zero\n",
    "flows['recovery_rate'] = flows.apply(\n",
    "    lambda row: round(row['recovery_flow'] / row['03/01/22'], 4) if row['03/01/22'] != 0 else 0, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:39:03.502900Z",
     "start_time": "2023-08-30T14:35:01.223313Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "date                     toid        mode classification  \\\n0        osgb4000000027865921         bus       Motorway   \n1        osgb4000000027865921         car       Motorway   \n2        osgb4000000027865921       cycle       Motorway   \n3        osgb4000000027865921  stationary       Motorway   \n4        osgb4000000027865921       total       Motorway   \n...                       ...         ...            ...   \n1729087  osgb5000005242182149         car     Local Road   \n1729088  osgb5000005242182149       cycle     Local Road   \n1729089  osgb5000005242182149  stationary     Local Road   \n1729090  osgb5000005242182149       total     Local Road   \n1729091  osgb5000005242182149       walks     Local Road   \n\ndate                                              geometry      Boundary  \\\n0        LINESTRING (531539.442 200769.874, 531592.988 ...  Outer London   \n1        LINESTRING (531539.442 200769.874, 531592.988 ...  Outer London   \n2        LINESTRING (531539.442 200769.874, 531592.988 ...  Outer London   \n3        LINESTRING (531539.442 200769.874, 531592.988 ...  Outer London   \n4        LINESTRING (531539.442 200769.874, 531592.988 ...  Outer London   \n...                                                    ...           ...   \n1729087  LINESTRING (543548.945 179236.408, 543554.000 ...  Inner London   \n1729088  LINESTRING (543548.945 179236.408, 543554.000 ...  Inner London   \n1729089  LINESTRING (543548.945 179236.408, 543554.000 ...  Inner London   \n1729090  LINESTRING (543548.945 179236.408, 543554.000 ...  Inner London   \n1729091  LINESTRING (543548.945 179236.408, 543554.000 ...  Inner London   \n\ndate    cycle_lane bus_lane   boroughs  03/01/22  02/22/22  ...  \\\n0                n        n    Enfield        16        11  ...   \n1                n        n    Enfield      1041      1100  ...   \n2                n        n    Enfield        14         4  ...   \n3                n        n    Enfield         2         0  ...   \n4                n        n    Enfield      1095      1151  ...   \n...            ...      ...        ...       ...       ...  ...   \n1729087          n        n  Greenwich        33        54  ...   \n1729088          n        n  Greenwich         2         1  ...   \n1729089          n        n  Greenwich         3         4  ...   \n1729090          n        n  Greenwich        50        74  ...   \n1729091          n        n  Greenwich        11        14  ...   \n\ndate     recovery_flow  impact_rate  recovery_rate  speed_03/01/22  \\\n0                   -4       0.4545        -0.2500        1.866527   \n1                   40      -0.0536         0.0384        1.699271   \n2                   -7       2.5000        -0.5000        1.884427   \n3                   -1       0.0000        -0.5000        1.004754   \n4                   27      -0.0487         0.0247        6.721938   \n...                ...          ...            ...             ...   \n1729087             18      -0.3889         0.5455        0.354515   \n1729088             -1       1.0000        -0.5000        0.000000   \n1729089              2      -0.2500         0.6667        0.000107   \n1729090             20      -0.3243         0.4000        0.773392   \n1729091              1      -0.2143         0.0909        0.077219   \n\ndate     speed_02/22/22  speed_03/08/22  impact_speed  recovery_speed  \\\n0              1.691628        1.966679      0.174899        0.100152   \n1              1.998224        1.852627     -0.298953        0.153356   \n2              1.883744        1.727556      0.000683       -0.156872   \n3              0.000000        0.000000      1.004754       -1.004754   \n4              5.940743        6.116473      0.781195       -0.605465   \n...                 ...             ...           ...             ...   \n1729087        0.357080        0.343335     -0.002565       -0.011180   \n1729088        0.264922        0.585180     -0.264922        0.585180   \n1729089        0.000151        0.000061     -0.000044       -0.000046   \n1729090        0.661585        1.025616      0.111807        0.252224   \n1729091        0.039433        0.071202      0.037785       -0.006017   \n\ndate     impact_speed_rate  recovery_speed_rate  \n0                   0.1034               0.0537  \n1                  -0.1496               0.0902  \n2                   0.0004              -0.0832  \n3                   0.0000              -1.0000  \n4                   0.1315              -0.0901  \n...                    ...                  ...  \n1729087            -0.0072              -0.0315  \n1729088            -1.0000               0.0000  \n1729089            -0.2902              -0.4319  \n1729090             0.1690               0.3261  \n1729091             0.9582              -0.0779  \n\n[1729092 rows x 22 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>date</th>\n      <th>toid</th>\n      <th>mode</th>\n      <th>classification</th>\n      <th>geometry</th>\n      <th>Boundary</th>\n      <th>cycle_lane</th>\n      <th>bus_lane</th>\n      <th>boroughs</th>\n      <th>03/01/22</th>\n      <th>02/22/22</th>\n      <th>...</th>\n      <th>recovery_flow</th>\n      <th>impact_rate</th>\n      <th>recovery_rate</th>\n      <th>speed_03/01/22</th>\n      <th>speed_02/22/22</th>\n      <th>speed_03/08/22</th>\n      <th>impact_speed</th>\n      <th>recovery_speed</th>\n      <th>impact_speed_rate</th>\n      <th>recovery_speed_rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>osgb4000000027865921</td>\n      <td>bus</td>\n      <td>Motorway</td>\n      <td>LINESTRING (531539.442 200769.874, 531592.988 ...</td>\n      <td>Outer London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Enfield</td>\n      <td>16</td>\n      <td>11</td>\n      <td>...</td>\n      <td>-4</td>\n      <td>0.4545</td>\n      <td>-0.2500</td>\n      <td>1.866527</td>\n      <td>1.691628</td>\n      <td>1.966679</td>\n      <td>0.174899</td>\n      <td>0.100152</td>\n      <td>0.1034</td>\n      <td>0.0537</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>osgb4000000027865921</td>\n      <td>car</td>\n      <td>Motorway</td>\n      <td>LINESTRING (531539.442 200769.874, 531592.988 ...</td>\n      <td>Outer London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Enfield</td>\n      <td>1041</td>\n      <td>1100</td>\n      <td>...</td>\n      <td>40</td>\n      <td>-0.0536</td>\n      <td>0.0384</td>\n      <td>1.699271</td>\n      <td>1.998224</td>\n      <td>1.852627</td>\n      <td>-0.298953</td>\n      <td>0.153356</td>\n      <td>-0.1496</td>\n      <td>0.0902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>osgb4000000027865921</td>\n      <td>cycle</td>\n      <td>Motorway</td>\n      <td>LINESTRING (531539.442 200769.874, 531592.988 ...</td>\n      <td>Outer London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Enfield</td>\n      <td>14</td>\n      <td>4</td>\n      <td>...</td>\n      <td>-7</td>\n      <td>2.5000</td>\n      <td>-0.5000</td>\n      <td>1.884427</td>\n      <td>1.883744</td>\n      <td>1.727556</td>\n      <td>0.000683</td>\n      <td>-0.156872</td>\n      <td>0.0004</td>\n      <td>-0.0832</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>osgb4000000027865921</td>\n      <td>stationary</td>\n      <td>Motorway</td>\n      <td>LINESTRING (531539.442 200769.874, 531592.988 ...</td>\n      <td>Outer London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Enfield</td>\n      <td>2</td>\n      <td>0</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>0.0000</td>\n      <td>-0.5000</td>\n      <td>1.004754</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.004754</td>\n      <td>-1.004754</td>\n      <td>0.0000</td>\n      <td>-1.0000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>osgb4000000027865921</td>\n      <td>total</td>\n      <td>Motorway</td>\n      <td>LINESTRING (531539.442 200769.874, 531592.988 ...</td>\n      <td>Outer London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Enfield</td>\n      <td>1095</td>\n      <td>1151</td>\n      <td>...</td>\n      <td>27</td>\n      <td>-0.0487</td>\n      <td>0.0247</td>\n      <td>6.721938</td>\n      <td>5.940743</td>\n      <td>6.116473</td>\n      <td>0.781195</td>\n      <td>-0.605465</td>\n      <td>0.1315</td>\n      <td>-0.0901</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1729087</th>\n      <td>osgb5000005242182149</td>\n      <td>car</td>\n      <td>Local Road</td>\n      <td>LINESTRING (543548.945 179236.408, 543554.000 ...</td>\n      <td>Inner London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Greenwich</td>\n      <td>33</td>\n      <td>54</td>\n      <td>...</td>\n      <td>18</td>\n      <td>-0.3889</td>\n      <td>0.5455</td>\n      <td>0.354515</td>\n      <td>0.357080</td>\n      <td>0.343335</td>\n      <td>-0.002565</td>\n      <td>-0.011180</td>\n      <td>-0.0072</td>\n      <td>-0.0315</td>\n    </tr>\n    <tr>\n      <th>1729088</th>\n      <td>osgb5000005242182149</td>\n      <td>cycle</td>\n      <td>Local Road</td>\n      <td>LINESTRING (543548.945 179236.408, 543554.000 ...</td>\n      <td>Inner London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Greenwich</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>-1</td>\n      <td>1.0000</td>\n      <td>-0.5000</td>\n      <td>0.000000</td>\n      <td>0.264922</td>\n      <td>0.585180</td>\n      <td>-0.264922</td>\n      <td>0.585180</td>\n      <td>-1.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>1729089</th>\n      <td>osgb5000005242182149</td>\n      <td>stationary</td>\n      <td>Local Road</td>\n      <td>LINESTRING (543548.945 179236.408, 543554.000 ...</td>\n      <td>Inner London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Greenwich</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>2</td>\n      <td>-0.2500</td>\n      <td>0.6667</td>\n      <td>0.000107</td>\n      <td>0.000151</td>\n      <td>0.000061</td>\n      <td>-0.000044</td>\n      <td>-0.000046</td>\n      <td>-0.2902</td>\n      <td>-0.4319</td>\n    </tr>\n    <tr>\n      <th>1729090</th>\n      <td>osgb5000005242182149</td>\n      <td>total</td>\n      <td>Local Road</td>\n      <td>LINESTRING (543548.945 179236.408, 543554.000 ...</td>\n      <td>Inner London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Greenwich</td>\n      <td>50</td>\n      <td>74</td>\n      <td>...</td>\n      <td>20</td>\n      <td>-0.3243</td>\n      <td>0.4000</td>\n      <td>0.773392</td>\n      <td>0.661585</td>\n      <td>1.025616</td>\n      <td>0.111807</td>\n      <td>0.252224</td>\n      <td>0.1690</td>\n      <td>0.3261</td>\n    </tr>\n    <tr>\n      <th>1729091</th>\n      <td>osgb5000005242182149</td>\n      <td>walks</td>\n      <td>Local Road</td>\n      <td>LINESTRING (543548.945 179236.408, 543554.000 ...</td>\n      <td>Inner London</td>\n      <td>n</td>\n      <td>n</td>\n      <td>Greenwich</td>\n      <td>11</td>\n      <td>14</td>\n      <td>...</td>\n      <td>1</td>\n      <td>-0.2143</td>\n      <td>0.0909</td>\n      <td>0.077219</td>\n      <td>0.039433</td>\n      <td>0.071202</td>\n      <td>0.037785</td>\n      <td>-0.006017</td>\n      <td>0.9582</td>\n      <td>-0.0779</td>\n    </tr>\n  </tbody>\n</table>\n<p>1729092 rows × 22 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speed = merge_csv_files(\n",
    "    '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Raw data/Traffic flow with speed')\n",
    "speed = speed.groupby(['travel_mode', 'toid', 'date']).agg({'speed_overall': 'mean'}).reset_index()\n",
    "speed = speed.pivot(index=['toid', 'travel_mode'], columns='date', values='speed_overall').reset_index()\n",
    "speed.rename(columns={'03/01/22': 'speed_03/01/22', '02/22/22': 'speed_02/22/22', '03/08/22': 'speed_03/08/22'},\n",
    "             inplace=True)\n",
    "columns_to_sum = ['speed_03/01/22', 'speed_02/22/22', 'speed_03/08/22']\n",
    "toid_mode_totals = speed.groupby(['toid', 'travel_mode'])[columns_to_sum].sum().reset_index()\n",
    "\n",
    "# 创建一个包含 'total' 行的 DataFrame\n",
    "total_row = toid_mode_totals.groupby('toid')[columns_to_sum].sum().reset_index()\n",
    "total_row['travel_mode'] = 'total'\n",
    "\n",
    "speed = pd.concat([toid_mode_totals, total_row], ignore_index=True)\n",
    "\n",
    "# Calculate the impact and recovery flows for one strike\n",
    "speed['impact_speed'] = speed['speed_03/01/22'] - speed['speed_02/22/22']\n",
    "speed['recovery_speed'] = speed['speed_03/08/22'] - speed['speed_03/01/22']\n",
    "\n",
    "speed['impact_speed_rate'] = speed.apply(\n",
    "    lambda row: round(row['impact_speed'] / row['speed_02/22/22'], 4) if row['speed_02/22/22'] != 0 else 0, axis=1)\n",
    "# Calculate recovery rate while avoiding division by zero\n",
    "speed['recovery_speed_rate'] = speed.apply(\n",
    "    lambda row: round(row['recovery_speed'] / row['speed_03/01/22'], 4) if row['speed_03/01/22'] != 0 else 0, axis=1)\n",
    "flows = pd.merge(flows, speed, left_on=['toid', 'mode'], right_on=['toid', 'travel_mode'], how='left')\n",
    "flows.drop(columns=['travel_mode'], inplace=True)\n",
    "flows"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:39:28.264914Z",
     "start_time": "2023-08-30T14:39:03.505940Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "All = flows.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T14:39:28.685589Z",
     "start_time": "2023-08-30T14:39:28.265663Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graph"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:39:26.483550Z",
     "start_time": "2023-08-13T20:38:54.981548Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_mode = 'stationary'\n",
    "flows = All[All['mode'] == selected_mode]\n",
    "flows.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creat a blank graph\n",
    "graph = nx.Graph()\n",
    "\n",
    "# iterate over the rows of the flows DataFrame\n",
    "for _, row in flows.iterrows():\n",
    "    mode = row['mode']\n",
    "    geometry = row['geometry']\n",
    "    baseline_1 = row['02/22/22']\n",
    "    strike_1 = row['03/01/22']\n",
    "    recovery_1 = row['03/08/22']\n",
    "    impact_flow = row['impact_flow']\n",
    "    recovery_flow = row['recovery_flow']\n",
    "    impact_rate = row['impact_rate']\n",
    "    recovery_rate = row['recovery_rate']\n",
    "    impact_speed = row['impact_speed']\n",
    "    recovery_speed = row['recovery_speed']\n",
    "    impact_speed_rate = row['impact_speed_rate']\n",
    "    recovery_speed_rate = row['recovery_speed_rate']\n",
    "\n",
    "    # break the MultiLineString geometry into its constituent LineStrings\n",
    "    if geometry.geom_type == 'MultiLineString':\n",
    "        for line_string in geometry.geoms:  # iterate over each LineString\n",
    "            from_node = line_string.coords[0]\n",
    "            to_node = line_string.coords[-1]\n",
    "\n",
    "            # Add nodes to the graph\n",
    "            graph.add_node(from_node, pos=from_node)  # Use 'from_node' as the node position\n",
    "            graph.add_node(to_node, pos=to_node)  # Use 'to_node' as the node position\n",
    "            # Add edges to the graph based on the direction\n",
    "\n",
    "            graph.add_edge(to_node, from_node, baseline_1=baseline_1, strike_1=strike_1,\n",
    "                           recovery_1=recovery_1, impact_flow=impact_flow, recovery_flow=recovery_flow,\n",
    "                           impact_rate=impact_rate, recovery_rate=recovery_rate, impact_speed_rate=impact_speed_rate,\n",
    "                           recovery_speed_rate=recovery_speed_rate, impact_speed=impact_speed,\n",
    "                           recovery_speed=recovery_speed, mode=mode,\n",
    "                           toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "\n",
    "            '''if direction == 'bothDirections':\n",
    "                # If the road is bidirectional, flows are split equally in both directions\n",
    "                graph.add_edge(from_node, to_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                               recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                               toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "                graph.add_edge(to_node,\n",
    "                               from_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2, recovery_1=recovery_1 / 2,\n",
    "                               impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                               toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "            elif direction == 'inOppositeDirection':\n",
    "            # If the road is in the opposite direction, flows are from the ending point to the starting point\n",
    "\n",
    "            elif direction == 'inDirection':\n",
    "                # If the road is in the same direction, flows are from the starting point to the ending point\n",
    "                graph.add_edge(from_node, to_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                               recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                               toid=row['toid'], classification=row['classification'], geometry=row['geometry'])'''\n",
    "\n",
    "    else:\n",
    "        from_node = geometry.coords[0]\n",
    "        to_node = geometry.coords[-1]\n",
    "\n",
    "        # Add nodes to the graph\n",
    "        graph.add_node(from_node, pos=from_node)  # Use 'from_node' as the node position\n",
    "        graph.add_node(to_node, pos=to_node)  # Use 'to_node' as the node position\n",
    "        # Add edges to the graph based on the direction\n",
    "\n",
    "        graph.add_edge(to_node, from_node, baseline_1=baseline_1, strike_1=strike_1, recovery_1=recovery_1,\n",
    "                       impact_flow=impact_flow, recovery_flow=recovery_flow, toid=row['toid'], impact_rate=impact_rate,\n",
    "                       recovery_rate=recovery_rate, impact_speed_rate=impact_speed_rate,\n",
    "                       recovery_speed_rate=recovery_speed_rate, impact_speed=impact_speed,\n",
    "                       recovery_speed=recovery_speed, mode=mode,\n",
    "                       classification=row['classification'], geometry=row['geometry'])\n",
    "\n",
    "    ''' \n",
    "     if direction == 'bothDirections':\n",
    "         # If the road is bidirectional, flows are split equally in both directions\n",
    "         graph.add_edge(from_node, to_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                        recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                        toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "         graph.add_edge(to_node, from_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                        recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                        toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "     elif direction == 'inOppositeDirection':\n",
    "         # If the road is in the opposite direction, flows are from the ending point to the starting point\n",
    "         graph.add_edge(to_node, from_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                        recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                        toid=row['toid'], classification=row['classification'], geometry=row['geometry'])\n",
    "     elif direction == 'inDirection':\n",
    "         # If the road is in the same direction, flows are from the starting point to the ending point\n",
    "         graph.add_edge(from_node, to_node, baseline_1=baseline_1 / 2, strike_1=strike_1 / 2,\n",
    "                        recovery_1=recovery_1 / 2, impact_flow=impact_flow / 2, recovery_flow=recovery_flow / 2,\n",
    "                        toid=row['toid'], classification=row['classification'], geometry=row['geometry'])'''\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "connected_components = list(nx.connected_components(graph))\n",
    "largest_component = max(connected_components, key=len)\n",
    "edges_list = list(graph.edges())\n",
    "\n",
    "# make sure the graph is not empty\n",
    "if edges_list:\n",
    "    # obtain the start and end coordinates of the first edge\n",
    "    u, v = edges_list[0]\n",
    "\n",
    "    # obtain the edge attribute of the first edge\n",
    "    edge_attr = graph[u][v]\n",
    "\n",
    "    print(f\"The first edge: ({u}, {v})\")\n",
    "    print(\"Attribute: \", edge_attr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# \n",
    "# def graph_visualization(graph, weight=None, cmap='Greens'):\n",
    "#     # obtain all the edges in the graph\n",
    "#     edges_list = list(graph.edges())\n",
    "# \n",
    "#     # make sure the graph is not empty\n",
    "#     if edges_list:\n",
    "#         # obtain the start and end coordinates of the first edge\n",
    "#         u, v = edges_list[0]\n",
    "# \n",
    "#         # obtain the edge attribute of the first edge\n",
    "#         edge_attr = graph[u][v]\n",
    "# \n",
    "#         print(f\"The first edge: ({u}, {v})\")\n",
    "#         print(\"Attribute: \", edge_attr)\n",
    "#     else:\n",
    "#         print(\"No edge in the graph.\")\n",
    "# \n",
    "#     # get the node positions\n",
    "#     node_positions = nx.get_node_attributes(graph, 'pos')\n",
    "# \n",
    "#     # get the edge weights\n",
    "#     edge_weight = nx.get_edge_attributes(graph, weight)\n",
    "# \n",
    "#     # normalize the edge weights between 0 and 1\n",
    "#     weight_values = list(edge_weight.values())\n",
    "#     norm = Normalize(vmin=min(weight_values), vmax=max(weight_values))\n",
    "#     norm_weight = {edge: norm(weight) for edge, weight in edge_weight.items()}\n",
    "# \n",
    "#     # create a colormap\n",
    "#     cmap_object = plt.get_cmap(cmap)\n",
    "#     mappable = ScalarMappable(norm=norm, cmap=cmap_object)\n",
    "#     mappable.set_array([])\n",
    "# \n",
    "#     # plot the graph\n",
    "#     fig, ax = plt.subplots()\n",
    "#     nx.draw_networkx_edges(graph, pos=node_positions, edge_color='gray')\n",
    "#     nx.draw_networkx_edges(graph, pos=node_positions,\n",
    "#                            edge_color=[mappable.to_rgba(norm_weight[edge]) for edge in edges_list])\n",
    "# \n",
    "#     # add a colorbar\n",
    "#     cbar = plt.colorbar(mappable, ax=ax, orientation='horizontal', pad=0.01)\n",
    "#     cbar.set_label(weight)\n",
    "# \n",
    "#     plt.title(f'Graph Representation of the Road Network', size=10)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# graph_visualization(graph, weight='impact_flow')\n",
    "# graph_visualization(graph, weight='recovery_flow')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Calculate the structure-based indicators for roads and system\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate the number of nodes\n",
    "nodes_number = graph.number_of_nodes()\n",
    "\n",
    "# calculate the number of edges\n",
    "links_number = graph.number_of_edges()\n",
    "\n",
    "# calculate the total link weight\n",
    "total_link_weight = sum([data['impact_flow'] for u, v, data in graph.edges(data=True)])\n",
    "\n",
    "mean_link_weight = total_link_weight / links_number\n",
    "\n",
    "# calculate the coefficient of variation of node degree\n",
    "node_degrees = dict(graph.degree())\n",
    "mean_node_degree = np.mean(list(node_degrees.values()))\n",
    "std_node_degree = np.std(list(node_degrees.values()))\n",
    "node_degree_cv = (std_node_degree / mean_node_degree) * 100\n",
    "\n",
    "# calculate the coefficient of variation of edge weight\n",
    "edge_weights = nx.get_edge_attributes(graph, 'impact_rate').values()\n",
    "mean_edge_weight = np.mean(list(edge_weights))\n",
    "std_edge_weight = np.std(list(edge_weights))\n",
    "edge_weight_cv = (std_edge_weight / mean_edge_weight) * 100\n",
    "\n",
    "# calculate the network connectivity and score for graph\n",
    "network_connectivity = nx.is_connected(graph)\n",
    "connectivity_score = 2 * links_number / (nodes_number * nodes_number)\n",
    "# calculate the average clustering coefficient for graph\n",
    "avg_clustering_coefficient = nx.average_clustering(graph)\n",
    "# calculate the transitivity for graph\n",
    "transitivity = nx.transitivity(graph)\n",
    "# calculate the assortativity for graph\n",
    "assortativity = nx.degree_assortativity_coefficient(graph)\n",
    "# calculate indicators as attributes for each road\n",
    "clustering_coefficients = nx.clustering(graph)\n",
    "# eigenvector_centrality = nx.eigenvector_centrality(graph)\n",
    "# calculate the node degrees\n",
    "node_degrees = graph.degree()\n",
    "\n",
    "# calculate the average degree for graph\n",
    "average_degree = sum(dict(node_degrees).values()) / len(node_degrees)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for u, v in graph.edges():\n",
    "    edge_clustering_coefficient = (clustering_coefficients[u] + clustering_coefficients[v]) / 2\n",
    "    graph[u][v]['clustering_coefficient'] = edge_clustering_coefficient\n",
    "    graph[u][v]['degree'] = (node_degrees[u] + node_degrees[v]) / 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import networkx as nx\n",
    "\n",
    "# Create a mapping from networkx nodes to consecutive integers\n",
    "node_to_index = {node: idx for idx, node in enumerate(graph.nodes())}\n",
    "\n",
    "# Convert networkx graph to igraph using edge list with mapped indices\n",
    "tuples = [(node_to_index[u], node_to_index[v]) for u, v in graph.edges()]\n",
    "ig_graph = ig.Graph.TupleList(tuples, directed=False)\n",
    "\n",
    "# Create a reverse mapping from igraph indices to networkx node identifiers\n",
    "index_to_node = {idx: node for node, idx in node_to_index.items()}\n",
    "\n",
    "# Calculate edge betweenness centrality using igraph\n",
    "edge_betweenness = ig_graph.edge_betweenness(directed=False, weights=None)\n",
    "\n",
    "# Ensure the order of edges in igraph matches the order in networkx\n",
    "ig_edges = [(index_to_node[edge.source], index_to_node[edge.target]) for edge in ig_graph.es]\n",
    "\n",
    "# Map betweenness values back to networkx edges\n",
    "for (u, v), betw in zip(ig_edges, edge_betweenness):\n",
    "    if (u, v) in graph.edges():\n",
    "        graph[u][v]['betweenness'] = betw\n",
    "    elif (v, u) in graph.edges():  # Handle undirected case\n",
    "        graph[v][u]['betweenness'] = betw\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "edges_list = list(graph.edges())\n",
    "\n",
    "# make sure the graph is not empty\n",
    "if edges_list:\n",
    "    # obtain the start and end coordinates of the first edge\n",
    "    u, v = edges_list[0]\n",
    "\n",
    "    # obtain the edge attribute of the first edge\n",
    "    edge_attr = graph[u][v]\n",
    "\n",
    "    print(f\"The first edge: ({u}, {v})\")\n",
    "    print(\"Attribute: \", edge_attr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the calculated indicators\n",
    "print(\"Total Nodes Number:\", nodes_number)\n",
    "print(\"Total Links Number:\", links_number)\n",
    "print(\"Total Flows:\", round(total_link_weight))\n",
    "print(\"Mean Link Flow:\", mean_link_weight)\n",
    "print(\"Node Degree Coefficient of Variation:\", node_degree_cv)\n",
    "print(\"Edge Weight Coefficient of Variation:\", edge_weight_cv)\n",
    "print(\"Connectivity Score:\", connectivity_score)\n",
    "print(\"Network Connectivity:\", network_connectivity)\n",
    "print(\"Transitivity:\", transitivity)\n",
    "print(\"Assortativity:\", assortativity)\n",
    "print(\"Average Clustering Coefficient:\", avg_clustering_coefficient)\n",
    "print(\"Average Degree:\", average_degree)\n",
    "print('Average Betweenness Centrality:', sum(nx.get_edge_attributes(graph, 'betweenness').values()) / len(\n",
    "    nx.get_edge_attributes(graph, 'betweenness').values()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(edge_betweenness))\n",
    "print(len(nx.get_edge_attributes(graph, 'toid').values()))\n",
    "\n",
    "mid_record = pd.DataFrame({'toid': nx.get_edge_attributes(graph, 'toid').values(),\n",
    "                           'betweenness': edge_betweenness,\n",
    "                           'clustering_coefficient': nx.get_edge_attributes(graph, 'clustering_coefficient').values(),\n",
    "                           'degree': nx.get_edge_attributes(graph, 'degree').values()\n",
    "                           })\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# graph_visualization(graph, weight='betweenness')\n",
    "# graph_visualization(graph, weight='clustering_coefficient')\n",
    "# graph_visualization(graph, weight='eigenvector_centrality')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network DBSCAN Clustering"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T20:43:23.112141Z",
     "start_time": "2023-08-13T20:43:23.111890Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# \n",
    "# # 将边的权重属性值作为数据点\n",
    "# edges_list = list(graph.edges())\n",
    "# data_points = np.array([graph[u][v]['impact_flow'] for u, v in edges_list]).reshape(-1, 1)\n",
    "# # 创建模型\n",
    "# dbscan = DBSCAN()\n",
    "# \n",
    "# # 定义要搜索的参数范围\n",
    "# param_grid = {'eps': [1, 1.5, 2, 2.5, 3, 4, 5, 6, 7, 8, 9, 10], 'min_samples': [1, 2, 3, 4, 5, 6]}\n",
    "# \n",
    "# best_score = -1\n",
    "# best_eps = None\n",
    "# best_min_samples = None\n",
    "# \n",
    "# # 在数据上执行交叉验证\n",
    "# for eps in param_grid['eps']:\n",
    "#     for min_samples in param_grid['min_samples']:\n",
    "#         dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "#         labels = dbscan.fit_predict(data_points)\n",
    "#         if len(set(labels)) > 1:  # 忽略只有一个簇的情况\n",
    "#             score = silhouette_score(data_points, labels)\n",
    "#             if score > best_score:\n",
    "#                 best_score = score\n",
    "#                 best_eps = eps\n",
    "#                 best_min_samples = min_samples\n",
    "# \n",
    "# print(\"Best EPS:\", best_eps)\n",
    "# print(\"Best Min Samples:\", best_min_samples)\n",
    "# \n",
    "# # 使用最佳参数进行DBSCAN聚类\n",
    "# best_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "# best_labels = best_dbscan.fit_predict(data_points)\n",
    "# \n",
    "# # 将聚类结果应用于图\n",
    "# for i, (u, v) in enumerate(edges_list):\n",
    "#     graph[u][v]['cluster_DB'] = best_labels[i]\n",
    "# \n",
    "# # 打印聚类的唯一值\n",
    "# unique_clusters = set(best_labels)\n",
    "# print(\"Unique Cluster Values:\", unique_clusters)\n",
    "# \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 创建颜色映射\n",
    "# cmap = plt.get_cmap('tab20', len(unique_clusters))\n",
    "# \n",
    "# # 绘制图形\n",
    "# fig, ax = plt.subplots()\n",
    "# pos_cluster = nx.fruchterman_reingold_layout(graph)\n",
    "# \n",
    "# for u, v, attr in graph.edges(data=True):\n",
    "#     cluster = attr['cluster_DB']\n",
    "#     if cluster == -1:\n",
    "#         edge_color = 'lightgrey'  # 将 -1 标签的边设置为浅灰色\n",
    "#     else:\n",
    "#         edge_color = cmap(cluster)\n",
    "#     nx.draw_networkx_edges(graph, pos_cluster, edgelist=[(u, v)], width=1, edge_color=edge_color)\n",
    "# \n",
    "# # 创建不连续的分类点图例\n",
    "# unique_labels = np.unique(best_labels)\n",
    "# handles = []\n",
    "# for label in unique_labels:\n",
    "#     if label == -1:  # 处理 -1 标签\n",
    "#         handle = plt.Line2D([], [], color='lightgrey', marker='o', markersize=10, label='Noise')\n",
    "#     else:\n",
    "#         color = cmap(label)\n",
    "#         handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "#     handles.append(handle)\n",
    "# \n",
    "# # 添加图例\n",
    "# ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "# \n",
    "# plt.axis('off')\n",
    "# plt.title(\"DBSCAN Clustering of Graph Edges\")\n",
    "# plt.show()\n",
    "# \n",
    "# node_positions = nx.get_node_attributes(graph, 'pos')\n",
    "# nx.draw_networkx_edges(graph, node_positions, width=1, edge_color=cmap(best_labels))\n",
    "# plt.title(\"DBSCAN Clustering in network\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Network K-Means Clustering\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:43:23.113660Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### By Degree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# \n",
    "# # 从图中获取节点的度作为特征\n",
    "# node_features = np.array([graph.degree(node) for node in graph.nodes()]).reshape(-1, 1)\n",
    "# # 使用 Z-Score 标准化对节点特征进行归一化\n",
    "# scaler = StandardScaler()\n",
    "# normalized_features = scaler.fit_transform(node_features)\n",
    "# \n",
    "# # 使用 Min-Max 缩放对节点特征进行归一化到 [0, 1] 范围\n",
    "# minmax_scaler = MinMaxScaler()\n",
    "# node_features = minmax_scaler.fit_transform(node_features)\n",
    "# \n",
    "# # 使用K均值算法进行聚类\n",
    "# k = 7  # 聚类数量\n",
    "# kmeans = KMeans(n_clusters=k)\n",
    "# cluster_labels = kmeans.fit_predict(node_features)\n",
    "# \n",
    "# # 将聚类结果应用于图\n",
    "# for i, node in enumerate(graph.nodes()):\n",
    "#     graph.nodes[node]['cluster_k'] = cluster_labels[i]\n",
    "# \n",
    "# # 初始化节点的累计flow和度的字典\n",
    "# node_weight = {node: 0 for node in graph.nodes()}\n",
    "# node_degrees = {node: 0 for node in graph.nodes()}\n",
    "# \n",
    "# # 遍历边，累计flow和度\n",
    "# for u, v, attr in graph.edges(data=True):\n",
    "#     weight = attr['impact_rate']\n",
    "#     node_weight[u] += weight\n",
    "#     node_weight[v] += weight\n",
    "#     node_degrees[u] += 1\n",
    "#     node_degrees[v] += 1\n",
    "# \n",
    "# # 计算每个节点的特征（平均flow）\n",
    "# node_features = np.array(\n",
    "#     [node_weight[node] / node_degrees[node] if node_degrees[node] > 0 else 0 for node in graph.nodes()]).reshape(-1, 1)\n",
    "# \n",
    "# # 创建颜色映射\n",
    "# cmap = plt.get_cmap('tab20', k)\n",
    "# \n",
    "# # 绘制图形\n",
    "# fig, ax = plt.subplots()\n",
    "# pos = nx.fruchterman_reingold_layout(graph)\n",
    "# \n",
    "# for u, v, attr in graph.edges(data=True):\n",
    "#     u_cluster = graph.nodes[u]['cluster_k']\n",
    "#     v_cluster = graph.nodes[v]['cluster_k']\n",
    "# \n",
    "#     if u_cluster == v_cluster:\n",
    "#         cluster = u_cluster\n",
    "#     else:\n",
    "#         cluster = -1  # 表示不同的聚类\n",
    "#     if cluster == -1:\n",
    "#         edge_color = 'lightgrey'  # 将不同聚类的边设置为浅灰色\n",
    "#     else:\n",
    "#         edge_color = cmap(cluster)\n",
    "#     nx.draw_networkx_edges(graph, pos_cluster, edgelist=[(u, v)], width=1, edge_color=edge_color)\n",
    "# \n",
    "# # 创建不连续的分类点图例\n",
    "# unique_labels = np.unique(cluster_labels)\n",
    "# handles = []\n",
    "# for label in unique_labels:\n",
    "#     if label == -1:  # 处理 -1 标签\n",
    "#         handle = plt.Line2D([], [], color='lightgrey', marker='o', markersize=10, label='Noise')\n",
    "#     else:\n",
    "#         color = cmap(label)\n",
    "#         handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "#     handles.append(handle)\n",
    "# \n",
    "# # 添加图例\n",
    "# ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1)).set_draggable(True)\n",
    "# \n",
    "# plt.axis('off')\n",
    "# plt.title(\"KMeans Clustering of Graph Nodes\")\n",
    "# plt.show()\n",
    "# \n",
    "# # Plot the clusters in the network\n",
    "# node_positions = nx.get_node_attributes(graph, 'pos')\n",
    "# nx.draw_networkx_edges(graph, node_positions, width=1, edge_color=cmap(cluster_labels))\n",
    "# # 创建不连续的分类点图例\n",
    "# unique_labels = np.unique(cluster_labels)\n",
    "# handles = []\n",
    "# for label in unique_labels:\n",
    "#     if label == -1:  # 处理 -1 标签\n",
    "#         handle = plt.Line2D([], [], color='lightgrey', marker='o', markersize=10, label='Noise')\n",
    "#     else:\n",
    "#         color = cmap(label)\n",
    "#         handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "#     handles.append(handle)\n",
    "# \n",
    "# # 添加图例\n",
    "# ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1)).set_draggable(True)\n",
    "# plt.title(\"KMeans Clustering in network\")\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### By Flow changes / Clustering Coefficient"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:43:23.116011Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 获取所有边的列表\n",
    "edges_list = list(graph.edges())\n",
    "\n",
    "# 按照flows聚类\n",
    "# 初始化特征矩阵\n",
    "num_edges = len(edges_list)\n",
    "feature_matrix = np.zeros((num_edges, 1))  # 1列，代表 \"flow\" 属性值\n",
    "\n",
    "# 填充特征矩阵\n",
    "for i, (u, v) in enumerate(edges_list):\n",
    "    flow = graph[u][v]['impact_rate']\n",
    "    feature_matrix[i, 0] = flow\n",
    "\n",
    "# 使用 Z-Score 标准化对节点特征进行归一化\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(feature_matrix)\n",
    "\n",
    "# 使用 Min-Max 缩放对节点特征进行归一化到 [0, 1] 范围\n",
    "minmax_scaler = MinMaxScaler()\n",
    "feature_matrix = minmax_scaler.fit_transform(feature_matrix)\n",
    "\n",
    "\n",
    "# #按照Clustering Coefficient聚类\n",
    "# # 初始化特征矩阵\n",
    "# num_edges = len(edges_list)\n",
    "# num_nodes = len(graph.nodes())\n",
    "# feature_matrix = np.zeros((num_edges, num_nodes))\n",
    "# \n",
    "# # 填充特征矩阵\n",
    "# for i, (u, v) in enumerate(edges_list):\n",
    "#     u_idx = list(graph.nodes()).index(u)  # 获取节点 u 的整数索引\n",
    "#     v_idx = list(graph.nodes()).index(v)  # 获取节点 v 的整数索引\n",
    "#     u_clustering = nx.clustering(graph)[u]\n",
    "#     v_clustering = nx.clustering(graph)[v]\n",
    "#     feature_matrix[i, u_idx] = u_clustering\n",
    "#     feature_matrix[i, v_idx] = v_clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 计算不同聚类数下的误差平方和\n",
    "inertia = []\n",
    "for k in range(1, 20):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(feature_matrix)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 20), inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Optimal Cluster Number for KMeans')\n",
    "plt.xticks(range(1, 20))  # 设置横轴刻度为整数\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 使用K均值算法进行聚类\n",
    "k = 7  # 聚类数量\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "# 将聚类结果应用于图\n",
    "for i, (u, v) in enumerate(edges_list):\n",
    "    graph[u][v]['Kmeans_bymode'] = cluster_labels[i]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# \n",
    "# # 创建颜色映射\n",
    "# cmap = plt.get_cmap('tab20', k)\n",
    "# \n",
    "# # 绘制图形\n",
    "# fig, ax = plt.subplots()\n",
    "# pos = nx.spring_layout(graph)\n",
    "# \n",
    "# for u, v, attr in graph.edges(data=True):\n",
    "#     cluster = attr['cluster_k']\n",
    "#     edge_color = cmap(cluster)\n",
    "#     nx.draw_networkx_edges(graph, pos, edgelist=[(u, v)], width=1, edge_color=edge_color)\n",
    "# \n",
    "# # 创建不连续的分类点图例\n",
    "# unique_labels = np.unique(cluster_labels)\n",
    "# handles = []\n",
    "# for label in unique_labels:\n",
    "#     color = cmap(label)\n",
    "#     handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "#     handles.append(handle)\n",
    "# \n",
    "# ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "# plt.axis('off')\n",
    "# plt.title(\"Edge Flow Changes-based Clustering\")\n",
    "# plt.show()\n",
    "# \n",
    "# # Plot the clusters in the network\n",
    "# node_positions = nx.get_node_attributes(graph, 'pos')\n",
    "# nx.draw_networkx_edges(graph, node_positions, width=1, edge_color=cmap(cluster_labels))\n",
    "# # 创建不连续的分类点图例\n",
    "# unique_labels = np.unique(cluster_labels)\n",
    "# handles = []\n",
    "# for label in unique_labels:\n",
    "#     if label == -1:  # 处理 -1 标签\n",
    "#         handle = plt.Line2D([], [], color='lightgrey', marker='o', markersize=10, label='Noise')\n",
    "#     else:\n",
    "#         color = cmap(label)\n",
    "#         handle = plt.Line2D([], [], color=color, marker='o', markersize=10, label=f'Cluster {label}')\n",
    "#     handles.append(handle)\n",
    "# \n",
    "# # 添加图例\n",
    "# ax.legend(handles=handles, title='Clusters', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "# plt.title(\"KMeans Clustering in network\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export the graph and update the indicators to All dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-13T20:22:49.295265Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# update = All.copy()\n",
    "# update = pd.merge(update, mid_record, on=['toid'], how='left')\n",
    "# data = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for _, _, edge_data in graph.edges(data=True):\n",
    "#     row_data = {\n",
    "#         'toid': edge_data.get('toid', None),\n",
    "#         'mode': edge_data.get('mode', None),\n",
    "#         'Kmeans_bymode': edge_data.get('Kmeans_bymode', None)\n",
    "#     }\n",
    "#     data.append(row_data)\n",
    "# \n",
    "# # 创建 DataFrame\n",
    "# graph_df = pd.DataFrame(data, columns=['toid', 'mode', 'Kmeans_bymode'])\n",
    "# graph_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # update['toid'] = update['toid'].astype(str)\n",
    "# update = pd.merge(update, graph_df, on=['toid', 'mode'], how='left')\n",
    "# update"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# update.to_csv('/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/processed data/All.csv',\n",
    "#               index=False)\n",
    "# update = gpd.GeoDataFrame(update, geometry='geometry')\n",
    "# update.to_file('/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/processed data/All.shp')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# update[update['mode'] == 'total'].to_file(\n",
    "#     '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/processed data/All/total.shp')\n",
    "# update[update['mode'] == 'car'].to_file(\n",
    "#     '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/processed data/All/car.shp')\n",
    "# update[update['mode'] == 'bus'].to_file(\n",
    "#     '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/processed data/All/bus.shp')\n",
    "# update[update['mode'] == 'cycle'].to_file(\n",
    "#     '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/processed data/All/cycle.shp')\n",
    "# update[update['mode'] == 'walks'].to_file(\n",
    "#     '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/processed data/All/walks.shp')\n",
    "# update[update['mode'] == 'stationary'].to_file(\n",
    "#     '/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/processed data/All/stationary.shp')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flow changes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Road space reallocation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.cm as cm\n",
    "import plotly.io as pio"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-30T16:22:41.344745Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flow_change = All.copy()\n",
    "flow_change.drop(\n",
    "    columns={'toid', 'geometry', 'impact_flow', 'recovery_flow', 'impact_rate', 'recovery_rate'},\n",
    "    inplace=True)\n",
    "\n",
    "flow_change = flow_change.groupby(['mode', 'classification', 'Boundary', 'boroughs']).agg(\n",
    "    {'03/01/22': 'sum', '02/22/22': 'sum', '03/08/22': 'sum', 'cycle_lane': 'first',\n",
    "     'bus_lane': 'first'}).reset_index().rename_axis(None, axis=1)\n",
    "flow_change = flow_change.astype({'03/01/22': int, '02/22/22': int, '03/08/22': int})\n",
    "flow_change = flow_change[flow_change['mode'] != 'total']\n",
    "flow_change.insert(0, 'Total Flows', 'Total Flows')\n",
    "flow_change['impact_flow'] = flow_change['03/01/22'] - flow_change['02/22/22']\n",
    "flow_change['recovery_flow'] = flow_change['03/08/22'] - flow_change['03/01/22']\n",
    "\n",
    "# Calculate impact rate while avoiding division by zero\n",
    "flow_change['impact_rate'] = flow_change.apply(\n",
    "    lambda row: round(row['impact_flow'] / row['02/22/22'], 4) if row['02/22/22'] != 0 else 0, axis=1)\n",
    "flow_change['recovery_rate'] = flow_change.apply(\n",
    "    lambda row: round(row['recovery_flow'] / row['03/01/22'], 4) if row['03/01/22'] != 0 else 0, axis=1)\n",
    "\n",
    "# 获取所有列的列表\n",
    "columns = flow_change.columns\n",
    "\n",
    "# 遍历每列，将内容转换为首字母大写\n",
    "for column in columns:\n",
    "    if flow_change[column].dtype == 'object':  # 仅对字符串列进行操作\n",
    "        flow_change[column] = flow_change[column].str.title()  # 使用str.title()函数将首字母大写\n",
    "\n",
    "flow_change.loc[flow_change['mode'] == 'Walks', 'mode'] = 'Walk'\n",
    "\n",
    "# 获取除了非数值列（例如日期和字符串）之外的所有列\n",
    "numeric_columns = flow_change.select_dtypes(include=['number']).columns\n",
    "\n",
    "# 创建MinMaxScaler对象\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 使用fit_transform方法对数值列进行缩放\n",
    "flow_change[numeric_columns] = scaler.fit_transform(flow_change[numeric_columns])\n",
    "\n",
    "flow_change"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "total = ['Total Flows']\n",
    "modes = ['Car', 'Bus', 'Cycle', 'Walk', 'Stationary']\n",
    "boundary_nodes = ['Motorway', 'Strategic Road', 'Local Road', 'Inner London', 'Outer London']\n",
    "\n",
    "nodes = total + modes + boundary_nodes\n",
    "node_indices = {node: index for index, node in enumerate(nodes)}\n",
    "\n",
    "dates = [\n",
    "    '03/01/22'\n",
    "    , \n",
    "    '02/22/22', '03/08/22'\n",
    "]\n",
    "\n",
    "\n",
    "# Function to convert HEX to RGBA\n",
    "def hex_to_rgba(hex_color, alpha=1):\n",
    "    hex_color = hex_color.lstrip('#')\n",
    "    h_len = len(hex_color)\n",
    "    return f'rgba({int(hex_color[0:h_len // 3], 16)}, {int(hex_color[h_len // 3:2 * h_len // 3], 16)}, {int(hex_color[2 * h_len // 3:h_len], 16)}, {alpha})'\n",
    "\n",
    "\n",
    "mode_colors = {\n",
    "    'Total Flows': '#604591',\n",
    "    'Bus': '#D88FA6',\n",
    "    'Car': '#CE684E',\n",
    "    'Cycle': '#477841',\n",
    "    'Walk': '#5A96D0',\n",
    "    'Stationary': '#A3A6A6',\n",
    "    'Motorway': '#E07F31',\n",
    "    'Strategic Road': '#357A3A',\n",
    "    'Local Road': '#CBCDCD',\n",
    "    'Inner London': '#7F7F7F',\n",
    "    'Outer London': '#000000'\n",
    "}\n",
    "\n",
    "# Initialize node_colors with None values\n",
    "node_colors = [None] * len(nodes)\n",
    "node_colors = [mode_colors.get(node, '#000000') for node in nodes]\n",
    "# Update node_colors\n",
    "for i, node in enumerate(nodes):\n",
    "    if node in mode_colors:\n",
    "        node_colors[i] = mode_colors[node]\n",
    "# Convert link_colors to RGBA with alpha value (for transparency)\n",
    "\n",
    "for date in dates:\n",
    "    sankey_data = []\n",
    "    for i, row in flow_change.iterrows():\n",
    "        # 第一流：Total Flows 到 modes\n",
    "        source_node = row['Total Flows']\n",
    "        target_node = row['mode']\n",
    "        value = row[date]\n",
    "\n",
    "        sankey_data.append({\n",
    "            'source': node_indices[source_node],\n",
    "            'target': node_indices[target_node],\n",
    "            'value': value,\n",
    "            'color': node_colors[node_indices[target_node]]  # 使用 target node 的颜色\n",
    "        })\n",
    "\n",
    "        # 第二流：modes 到 classification\n",
    "        source_node = row['mode']\n",
    "        target_node = row['classification']\n",
    "        value = row[date]\n",
    "\n",
    "        sankey_data.append({\n",
    "            'source': node_indices[source_node],\n",
    "            'target': node_indices[target_node],\n",
    "            'value': value,\n",
    "            'color': node_colors[node_indices[source_node]]  # 使用 source node 的颜色\n",
    "        })\n",
    "\n",
    "        # 第三流：classification 到 Boundary\n",
    "        source_node = row['classification']\n",
    "        target_node = row['Boundary']\n",
    "        value = row[date]\n",
    "\n",
    "        sankey_data.append({\n",
    "            'source': node_indices[source_node],\n",
    "            'target': node_indices[target_node],\n",
    "            'value': value,\n",
    "            'color': node_colors[node_indices[source_node]]  # 使用 source node 的颜色\n",
    "        })\n",
    "\n",
    "    # Create a new list to store the link colors, using the logic you described\n",
    "    link_colors = []\n",
    "    for link in sankey_data:\n",
    "        if link.get('type') == 'total_to_mode':\n",
    "            link_colors.append(node_colors[link['target']])\n",
    "        else:\n",
    "            link_colors.append(node_colors[link['source']])\n",
    "\n",
    "    link_colors_rgba = [hex_to_rgba(color, alpha=0.6) for color in link_colors]\n",
    "\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='freeform',\n",
    "        node=dict(\n",
    "            pad=10,\n",
    "            thickness=20,\n",
    "            line=dict(color='black', width=0.3),\n",
    "            label=nodes,\n",
    "            color=node_colors\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=[link['source'] for link in sankey_data],\n",
    "            target=[link['target'] for link in sankey_data],\n",
    "            value=[link['value'] for link in sankey_data],\n",
    "            color=link_colors_rgba  # Use the link_colors list you just created\n",
    "        ),\n",
    "    ))\n",
    "\n",
    "    safe_date = date.replace('/', '-')\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Road Space Allocation on {safe_date}\",\n",
    "        font_size=15,\n",
    "        autosize=False,\n",
    "        hovermode='closest',\n",
    "        width=1600,  # 设置图像宽度\n",
    "        height=600,  # 设置图像高度\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    fig.write_image(\n",
    "        f'/Users/zonghe/Library/CloudStorage/OneDrive-UniversityCollegeLondon/Zonghe Ma/Output/Print/sankey_diagram on {safe_date}.png',\n",
    "        scale=3)  # 设置分辨率为原始大小的两倍\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_mode_t = All.copy()\n",
    "\n",
    "class_mode_t = class_mode_t.groupby(['mode', 'classification', 'Boundary']).agg(\n",
    "    {'03/01/22': 'sum', '02/22/22': 'sum', '03/08/22': 'sum', 'cycle_lane': 'first',\n",
    "     'bus_lane': 'first'}).reset_index().rename_axis(None, axis=1)\n",
    "class_mode_t = class_mode_t.astype({'03/01/22': int, '02/22/22': int, '03/08/22': int})\n",
    "\n",
    "class_mode_t['impact_flow'] = class_mode_t['03/01/22'] - class_mode_t['02/22/22']\n",
    "class_mode_t['recovery_flow'] = class_mode_t['03/08/22'] - class_mode_t['03/01/22']\n",
    "# Calculate impact rate while avoiding division by zero\n",
    "class_mode_t['impact_rate'] = class_mode_t.apply(\n",
    "    lambda row: round(row['impact_flow'] / row['02/22/22'], 4) if row['02/22/22'] != 0 else 0, axis=1)\n",
    "# Calculate recovery rate while avoiding division by zero\n",
    "class_mode_t['recovery_rate'] = class_mode_t.apply(\n",
    "    lambda row: round(row['recovery_flow'] / row['03/01/22'], 4) if row['03/01/22'] != 0 else 0, axis=1)\n",
    "\n",
    "# 获取所有列的列表\n",
    "columns = class_mode_t.columns\n",
    "\n",
    "# 遍历每列，将内容转换为首字母大写\n",
    "for column in columns:\n",
    "    if class_mode_t[column].dtype == 'object':  # 仅对字符串列进行操作\n",
    "        class_mode_t[column] = class_mode_t[column].str.title()  # 使用str.title()函数将首字母大写\n",
    "\n",
    "class_mode_t = class_mode_t.pivot_table(index='classification', columns='mode', values=['impact_flow', 'recovery_flow'])\n",
    "# 将列名重新整理成多重索引的形式\n",
    "class_mode_t.columns = [f'{col[1]}/{col[0]}' for col in class_mode_t.columns]\n",
    "\n",
    "class_mode_t"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_mode_t2 = All.copy()\n",
    "\n",
    "class_mode_t2 = class_mode_t2.groupby(['mode', 'classification', 'Boundary', 'boroughs']).agg(\n",
    "    {'03/01/22': 'sum', '02/22/22': 'sum', '03/08/22': 'sum', 'cycle_lane': 'first',\n",
    "     'bus_lane': 'first'}).reset_index().rename_axis(None, axis=1)\n",
    "class_mode_t2 = class_mode_t2.astype({'03/01/22': int, '02/22/22': int, '03/08/22': int})\n",
    "\n",
    "class_mode_t2['impact_flow'] = class_mode_t2['03/01/22'] - class_mode_t2['02/22/22']\n",
    "class_mode_t2['recovery_flow'] = class_mode_t2['03/08/22'] - class_mode_t2['03/01/22']\n",
    "# Calculate impact rate while avoiding division by zero\n",
    "class_mode_t2['impact_rate'] = class_mode_t2.apply(\n",
    "    lambda row: round(row['impact_flow'] / row['02/22/22'], 4) if row['02/22/22'] != 0 else 0, axis=1)\n",
    "# Calculate recovery rate while avoiding division by zero\n",
    "class_mode_t2['recovery_rate'] = class_mode_t2.apply(\n",
    "    lambda row: round(row['recovery_flow'] / row['03/01/22'], 4) if row['03/01/22'] != 0 else 0, axis=1)\n",
    "\n",
    "# 获取所有列的列表\n",
    "columns = class_mode_t2.columns\n",
    "\n",
    "# 遍历每列，将内容转换为首字母大写\n",
    "for column in columns:\n",
    "    if class_mode_t2[column].dtype == 'object':  # 仅对字符串列进行操作\n",
    "        class_mode_t2[column] = class_mode_t2[column].str.title()  # 使用str.title()函数将首字母大写\n",
    "\n",
    "class_mode_t2 = class_mode_t2.pivot_table(index=['boroughs', 'classification'], columns='mode',\n",
    "                                          values=['impact_rate', 'recovery_rate'])\n",
    "# 将列名重新整理成多重索引的形式\n",
    "# class_mode_t2.columns = [f'{col[1]}/{col[0]}' for col in class_mode_t.columns]\n",
    "# class_mode_t2.index = [f'{col[1]}/{col[0]}' for col in class_mode_t.index]\n",
    "\n",
    "class_mode_t2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "mode                                      Total                       Car  \\\ndate                                impact_rate recovery_rate impact_rate   \nclassification toid                                                         \nLocal Road     Osgb4000000027869139      0.0416        0.1171      0.0403   \n               Osgb4000000027869149      0.0000        0.0000      0.0000   \n               Osgb4000000027876087     -1.0000        0.0000     -1.0000   \n               Osgb4000000027876088     -0.1105        0.1893     -0.1047   \n               Osgb4000000027876095      0.1910        0.0188      0.2046   \n...                                         ...           ...         ...   \nStrategic Road Osgb5000005241746776     -0.0168       -0.0546     -0.0504   \n               Osgb5000005241746777      0.0000        0.0000      0.0000   \n               Osgb5000005241864717     -0.0963        0.0492     -0.0577   \n               Osgb5000005242071802      0.2846        0.4051      0.2212   \n               Osgb5000005242156433     -0.1752        0.0490     -0.1510   \n\nmode                                                      Bus                \\\ndate                                recovery_rate impact_rate recovery_rate   \nclassification toid                                                           \nLocal Road     Osgb4000000027869139        0.1108     -1.0000           0.0   \n               Osgb4000000027869149        0.0000      0.0000           0.0   \n               Osgb4000000027876087        0.0000      0.0000           0.0   \n               Osgb4000000027876088        0.2532      0.0000          -0.5   \n               Osgb4000000027876095        0.0174      0.2000          -1.0   \n...                                           ...         ...           ...   \nStrategic Road Osgb5000005241746776       -0.0076      4.0000          -0.2   \n               Osgb5000005241746777        0.0000      0.0000           0.0   \n               Osgb5000005241864717        0.0510     -0.5000           1.0   \n               Osgb5000005242071802        0.3188      0.3333           0.5   \n               Osgb5000005242156433        0.0198     -0.4615           1.0   \n\nmode                                      Cycle                     Walks  \\\ndate                                impact_rate recovery_rate impact_rate   \nclassification toid                                                         \nLocal Road     Osgb4000000027869139      0.0000       -1.0000      0.0000   \n               Osgb4000000027869149      0.0000        0.0000      0.0000   \n               Osgb4000000027876087      0.0000        0.0000     -1.0000   \n               Osgb4000000027876088      0.0000        0.0000     -0.1538   \n               Osgb4000000027876095     -0.8750        1.0000     -0.5000   \n...                                         ...           ...         ...   \nStrategic Road Osgb5000005241746776      0.2000       -0.3333      0.2857   \n               Osgb5000005241746777      0.0000        0.0000      0.0000   \n               Osgb5000005241864717      1.6667       -0.8750     -0.4000   \n               Osgb5000005242071802      2.2500        0.4615      0.0000   \n               Osgb5000005242156433     -0.4737        1.5000     -0.1951   \n\nmode                                               Stationary                \ndate                                recovery_rate impact_rate recovery_rate  \nclassification toid                                                          \nLocal Road     Osgb4000000027869139        1.5000      0.0000        0.0000  \n               Osgb4000000027869149        0.0000      0.0000        0.0000  \n               Osgb4000000027876087        0.0000      0.0000        0.0000  \n               Osgb4000000027876088       -0.7273     -0.3333        1.0000  \n               Osgb4000000027876095        3.0000     -1.0000        0.0000  \n...                                           ...         ...           ...  \nStrategic Road Osgb5000005241746776       -0.6111      0.0000        0.0000  \n               Osgb5000005241746777        0.0000      0.0000        0.0000  \n               Osgb5000005241864717        0.4000     -1.0000        0.0000  \n               Osgb5000005242071802        4.0000      0.0000        0.0000  \n               Osgb5000005242156433       -0.3030      0.0000       -0.6667  \n\n[288174 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>mode</th>\n      <th colspan=\"2\" halign=\"left\">Total</th>\n      <th colspan=\"2\" halign=\"left\">Car</th>\n      <th colspan=\"2\" halign=\"left\">Bus</th>\n      <th colspan=\"2\" halign=\"left\">Cycle</th>\n      <th colspan=\"2\" halign=\"left\">Walks</th>\n      <th colspan=\"2\" halign=\"left\">Stationary</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>date</th>\n      <th>impact_rate</th>\n      <th>recovery_rate</th>\n      <th>impact_rate</th>\n      <th>recovery_rate</th>\n      <th>impact_rate</th>\n      <th>recovery_rate</th>\n      <th>impact_rate</th>\n      <th>recovery_rate</th>\n      <th>impact_rate</th>\n      <th>recovery_rate</th>\n      <th>impact_rate</th>\n      <th>recovery_rate</th>\n    </tr>\n    <tr>\n      <th>classification</th>\n      <th>toid</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">Local Road</th>\n      <th>Osgb4000000027869139</th>\n      <td>0.0416</td>\n      <td>0.1171</td>\n      <td>0.0403</td>\n      <td>0.1108</td>\n      <td>-1.0000</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>-1.0000</td>\n      <td>0.0000</td>\n      <td>1.5000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>Osgb4000000027869149</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>Osgb4000000027876087</th>\n      <td>-1.0000</td>\n      <td>0.0000</td>\n      <td>-1.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>-1.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>Osgb4000000027876088</th>\n      <td>-0.1105</td>\n      <td>0.1893</td>\n      <td>-0.1047</td>\n      <td>0.2532</td>\n      <td>0.0000</td>\n      <td>-0.5</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>-0.1538</td>\n      <td>-0.7273</td>\n      <td>-0.3333</td>\n      <td>1.0000</td>\n    </tr>\n    <tr>\n      <th>Osgb4000000027876095</th>\n      <td>0.1910</td>\n      <td>0.0188</td>\n      <td>0.2046</td>\n      <td>0.0174</td>\n      <td>0.2000</td>\n      <td>-1.0</td>\n      <td>-0.8750</td>\n      <td>1.0000</td>\n      <td>-0.5000</td>\n      <td>3.0000</td>\n      <td>-1.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">Strategic Road</th>\n      <th>Osgb5000005241746776</th>\n      <td>-0.0168</td>\n      <td>-0.0546</td>\n      <td>-0.0504</td>\n      <td>-0.0076</td>\n      <td>4.0000</td>\n      <td>-0.2</td>\n      <td>0.2000</td>\n      <td>-0.3333</td>\n      <td>0.2857</td>\n      <td>-0.6111</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>Osgb5000005241746777</th>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>Osgb5000005241864717</th>\n      <td>-0.0963</td>\n      <td>0.0492</td>\n      <td>-0.0577</td>\n      <td>0.0510</td>\n      <td>-0.5000</td>\n      <td>1.0</td>\n      <td>1.6667</td>\n      <td>-0.8750</td>\n      <td>-0.4000</td>\n      <td>0.4000</td>\n      <td>-1.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>Osgb5000005242071802</th>\n      <td>0.2846</td>\n      <td>0.4051</td>\n      <td>0.2212</td>\n      <td>0.3188</td>\n      <td>0.3333</td>\n      <td>0.5</td>\n      <td>2.2500</td>\n      <td>0.4615</td>\n      <td>0.0000</td>\n      <td>4.0000</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n    <tr>\n      <th>Osgb5000005242156433</th>\n      <td>-0.1752</td>\n      <td>0.0490</td>\n      <td>-0.1510</td>\n      <td>0.0198</td>\n      <td>-0.4615</td>\n      <td>1.0</td>\n      <td>-0.4737</td>\n      <td>1.5000</td>\n      <td>-0.1951</td>\n      <td>-0.3030</td>\n      <td>0.0000</td>\n      <td>-0.6667</td>\n    </tr>\n  </tbody>\n</table>\n<p>288174 rows × 12 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mode_f = All.copy()\n",
    "# 获取所有列的列表\n",
    "columns = class_mode_f.columns\n",
    "\n",
    "# 遍历每列，将内容转换为首字母大写\n",
    "for column in columns:\n",
    "    if class_mode_f[column].dtype == 'object':  # 仅对字符串列进行操作\n",
    "        class_mode_f[column] = class_mode_f[column].str.title()  # 使用str.title()函数将首字母大写\n",
    "\n",
    "class_mode_f = class_mode_f.pivot_table(index=['classification', 'toid'], values=['impact_rate', 'recovery_rate'],\n",
    "                                        columns='mode')\n",
    "# 将列名重新整理成多重索引的形式\n",
    "# class_mode_f.columns = [f'{col[1]}/{col[0]}' for col in class_mode_f.columns]\n",
    "class_mode_f = class_mode_f.swaplevel(axis=1)\n",
    "\n",
    "# 定义第一层和第二层索引的顺序\n",
    "first_level_order = ['Total', 'Car', 'Bus', 'Cycle', 'Walks', 'Stationary']\n",
    "second_level_order = ['impact_rate', 'recovery_rate']\n",
    "\n",
    "# 初始化一个空列表用于存储排序后的列名\n",
    "sorted_columns = []\n",
    "\n",
    "# 循环遍历第一层索引的顺序\n",
    "for first_level in first_level_order:\n",
    "    # 循环遍历第二层索引的顺序\n",
    "    for second_level in second_level_order:\n",
    "        # 构建当前列名\n",
    "        current_column = (first_level, second_level)\n",
    "        # 将当前列名添加到排序后的列名列表中\n",
    "        sorted_columns.append(current_column)\n",
    "\n",
    "# 使用排序后的列名对 DataFrame 进行排序\n",
    "class_mode_f = class_mode_f[sorted_columns]\n",
    "\n",
    "class_mode_f\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:22:49.036784Z",
     "start_time": "2023-08-30T16:22:44.296904Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n",
    "# 选择需要进行归一化的数值列\n",
    "numeric_columns = class_mode_f.select_dtypes(include=['number']).columns\n",
    "\n",
    "# 创建MinMaxScaler对象\n",
    "\n",
    "scaler_positive = MinMaxScaler(feature_range=(0, 1))  # 归一化到区间[0, 1]\n",
    "scaler_negative = MinMaxScaler(feature_range=(-1, 0))  # 归一化到区间[-1, 0]\n",
    "\n",
    "# 对大于等于0的数据进行归一化\n",
    "class_mode_f_positive = class_mode_f.copy()\n",
    "positive_values = class_mode_f[numeric_columns].values\n",
    "positive_mask = positive_values >= 0\n",
    "class_mode_f[numeric_columns] = np.where(positive_mask, scaler_positive.fit_transform(positive_values), positive_values)\n",
    "\n",
    "# 对小于0的数据进行归一化\n",
    "class_mode_f_negative = class_mode_f.copy()\n",
    "negative_values = class_mode_f[numeric_columns].values\n",
    "negative_mask = positive_values < 0\n",
    "class_mode_f[numeric_columns] = np.where(negative_mask, scaler_negative.fit_transform(negative_values), negative_values)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T16:22:49.167589Z",
     "start_time": "2023-08-30T16:22:49.038120Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class_mode_f = pd.DataFrame(np.random.rand(10, 10))\n",
    "\n",
    "# Create the figure and axis\n",
    "figure, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "# Create a heatmap using Seaborn\n",
    "sns.heatmap(class_mode_f, cmap='viridis', annot=True, ax=ax)\n",
    "\n",
    "# Set title\n",
    "ax.set_title(\"Impact and Recovery Flow for Multi-modes by Road Classification\")\n",
    "\n",
    "# Set x-axis tick labels and rotation\n",
    "ax.set_xticks(range(len(class_mode_f.columns)))\n",
    "ax.set_xticklabels(class_mode_f.columns, rotation=30)\n",
    "\n",
    "# Assuming classifications are a list of strings. Replace with your actual classifications.\n",
    "classifications = ['Motorway', 'Strategic Road', 'Local Road']\n",
    "\n",
    "# Set y-axis tick labels and classifications\n",
    "ax.set_yticks(range(len(classifications)))\n",
    "ax.set_yticklabels(classifications)\n",
    "\n",
    "# Get unique classifications and counts\n",
    "unique_classifications, counts = np.unique(class_mode_f.index, return_counts=True)\n",
    "grouped_ticks = np.cumsum(counts) - counts / 2\n",
    "lines_ticks = np.cumsum(counts)\n",
    "\n",
    "# Set left y-axis ticks\n",
    "ax.set_yticks(grouped_ticks)\n",
    "ax.set_yticklabels(unique_classifications)\n",
    "\n",
    "# Add y-axis dividing lines\n",
    "for tick in lines_ticks[:-1]:\n",
    "    ax.axhline(tick, color='white', linewidth=1.2)\n",
    "\n",
    "# Remove default grid lines\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
